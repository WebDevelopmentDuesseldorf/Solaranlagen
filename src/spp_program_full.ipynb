{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import packages\r\n",
    "from numpy.core.numeric import NaN\r\n",
    "from numpy.lib.arraypad import _set_reflect_both\r\n",
    "from numpy.lib.utils import byte_bounds\r\n",
    "from pandas.core.frame import DataFrame\r\n",
    "import requests\r\n",
    "import datetime as dt\r\n",
    "import pandas as pd\r\n",
    "import json\r\n",
    "from requests.api import request\r\n",
    "from geopy.geocoders import Nominatim\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "import bisect\r\n",
    "from scipy.stats import zscore\r\n",
    "from numpy import add\r\n",
    "from d01_data.get_weather_data import getload_weather, get_solar_irradiance\r\n",
    "from d01_data.get_address_data import get_reference_data as getref\r\n",
    "import d01_data.get_topo_data as topo_raw\r\n",
    "import d02_intermediate.create_topo_int as topo_int\r\n",
    "import d03_processing.topo_process as topo_pro\r\n",
    "from d02_intermediate.est_albedo import albedo\r\n",
    "import os\r\n",
    "from os.path import dirname, abspath\r\n",
    "import sys \r\n",
    "import d00_utils.id_management as idmanage\r\n",
    "from ast import literal_eval as make_tuple\r\n",
    "import pickle\r\n",
    "import d02_intermediate.geometry3d as sh\r\n",
    "import d07_visualisation.create_geometries as creategeo\r\n",
    "import geopandas as gpd\r\n",
    "from itertools import product\r\n",
    "from geojson import Feature, Point, Polygon, MultiPolygon, FeatureCollection\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import shapely.geometry as sg\r\n",
    "from d02_intermediate.geometry3d import inc_angle\r\n",
    "from d02_intermediate.est_incidence_on_panel import radiation_incidence_on_panel as incidence_on_panel\r\n",
    "# ignore pd warnings\r\n",
    "pd.options.mode.chained_assignment = None "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# set parameters for energy consumption\r\n",
    "# might be used to calculate optimal locations\r\n",
    "use_standard_consumption = True\r\n",
    "# use standard values for germany if needed\r\n",
    "if use_standard_consumption:\r\n",
    "    # set average energy consumption for june and january in kWh(source: https://musterhaushalt.de)\r\n",
    "    power_usage_january_total = 299\r\n",
    "    power_usage_july_total = 249\r\n",
    "    # set energy consumption values by times of day in Wh\r\n",
    "    # consumption night from 24:00 to 06:00\r\n",
    "    consumption_night = 14*70\r\n",
    "    # morning = 06:00 to 12:00\r\n",
    "    consumption_morning = 36*70\r\n",
    "    # afternoon = 12:00 to 18:00\r\n",
    "    consumption_afternoon = 36*70\r\n",
    "    # evening = 18:00 to 24:00\r\n",
    "    consumption_evening = 43*70\r\n",
    "    # set the month of the measuring date for the times of day\r\n",
    "    measuring_month = 'september'\r\n",
    "# find the fitting category for the consumption profile: consumption_type\r\n",
    "consumption_type = 'max total'\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# set some panel parameters\r\n",
    "#  set if the panel mounts can adjust the tilt and alignment\r\n",
    "adjustable_mounts = True\r\n",
    "# set panel sizes in mm\r\n",
    "panel_length = 1660\r\n",
    "panel_width = 990\r\n",
    "# set the adjustable range in mm: mount_leeway\r\n",
    "mount_leeway = 400\r\n",
    "if adjustable_mounts:\r\n",
    "    max_tilt_adj = math.degrees(math.asin(mount_leeway/panel_length))\r\n",
    "    max_align_adj = math.degrees(math.asin(mount_leeway/panel_width))\r\n",
    "\r\n",
    "# temperature coefficient P_mpp or gamma\r\n",
    "temp_coeff = -.43\r\n",
    "# compute losses by technical issues (Inverter, mismatch, LID, age (15y))\r\n",
    "loss_technical = 1-(.97*.98*.985*.9625)\r\n",
    "# set panel efficiency: panel_efficiency (under STC)\r\n",
    "panel_efficiency = .17"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# set id for round (string)\r\n",
    "process_id = 'test'\r\n",
    "# choose if locally saved data will be used (if available)\r\n",
    "use_available = True\r\n",
    "# set degree delta for resolution of topo requests, don't change this value, other values are not possible currently\r\n",
    "airmaps_degree_delta = float(0.000277778)\r\n",
    "# choose the locations\r\n",
    "location = 'thÃ¼ringen'\r\n",
    "# choose the resolution\r\n",
    "resolution = 24\r\n",
    "# create an updated process_id\r\n",
    "process_id = process_id+'_'+location+'_res_'+str(resolution)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# create a reference dict for the location\r\n",
    "# info from the dict will be used for the request of weather data e.g.\r\n",
    "reference_dict = getref(location)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# update the location dictionary\r\n",
    "# set if the update will be forced\r\n",
    "force_update_dict = False\r\n",
    "# set if a new dict has to be created (the old one gets deleted -_- ) \r\n",
    "create_new_dict = False\r\n",
    "# set the load/save path\r\n",
    "path = '../references/location_dictionary.pkl'\r\n",
    "# load the current dict if available\r\n",
    "if os.path.exists(path) and not create_new_dict:\r\n",
    "    with open(path,'rb') as f:\r\n",
    "        loc_dict=pickle.load(f)\r\n",
    "else:\r\n",
    "    loc_dict = {}\r\n",
    "\r\n",
    "# check if the location is already included in the dict\r\n",
    "if (not location in loc_dict) or force_update_dict:\r\n",
    "    # if the location is new, add the reference data\r\n",
    "    loc_dict[location] = getref(location)\r\n",
    "    # save it\r\n",
    "    with open(path,'wb') as f:\r\n",
    "        pickle.dump(loc_dict,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# import weather data\r\n",
    "weather_df = getload_weather(process_id,location=location,use_available=True, resolution=resolution)\r\n",
    "# drop useless columns\r\n",
    "weather_dropped_df = weather_df.drop(['lat','lon','id'],axis=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "available weather data will be loaded. change use_available or process_id to request new data\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# code in this cell is outdated, kept for documentation purposes\r\n",
    "\r\n",
    "# # drop rows with zero radiation\r\n",
    "# # find all rows that correspond to a time of zero radiation\r\n",
    "# is_rad = weather_df['parameter']=='global_rad:W'\r\n",
    "# is_zero = weather_df['value']==0\r\n",
    "# # save the data to drop\r\n",
    "# to_drop = weather_df[is_zero & is_rad]\r\n",
    "# # drop the rows\r\n",
    "# weather_dropped_df = weather_df.drop(to_drop.index)\r\n",
    "# # drop unnecessary coordinates from weather_dropped_df\r\n",
    "# weather_dropped_df.drop(['lat','lon','id'],inplace=True,axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# prepare dataframe to save all ids and block statuses\r\n",
    "ids_all_df = idmanage.id_manage_df(weather_dropped_df,id_col='id_tuple')\r\n",
    "ids_allowed = idmanage.id_allow_df(ids_all_df)\r\n",
    "# empty block dataframe\r\n",
    "block_df = pd.DataFrame(columns=['id_tuple','blocked','block reason'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# block locations outside of the chosen area\r\n",
    "\r\n",
    "# access reference polygon: reference\r\n",
    "reference = loc_dict[location]['polygon']\r\n",
    "\r\n",
    "# create a dataframe with currently allowed ids\r\n",
    "loc_gdf = pd.DataFrame(ids_allowed)\r\n",
    "# add a Point column with shapely Points\r\n",
    "loc_gdf['geometry'] = loc_gdf.apply(lambda x: sg.Point((x['id_tuple'][1],x['id_tuple'][0])), axis=1)\r\n",
    "# update the block_df\r\n",
    "block_df = idmanage.update_id_block_df(loc_gdf,block_df,'outside of',reference,'outside of '+location, block_col='geometry')\r\n",
    "# update the other id management structures\r\n",
    "ids_all_df = idmanage.updateblocks_idmanage_df(block_df, ids_all_df)\r\n",
    "ids_allowed = idmanage.id_allow_df(ids_all_df)\r\n",
    "# drop weather data from blocked locations\r\n",
    "weather_dropped_df = weather_dropped_df[weather_dropped_df['id_tuple'].isin(ids_allowed)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# reformat the dataframe so it has a single row per time and location with all of the weather data\r\n",
    "# get a list of the unique parameters\r\n",
    "parameters = weather_dropped_df.parameter.unique()\r\n",
    "# create a sub_df for for each parameter\r\n",
    "# collect subs in a list: sub_df_list\r\n",
    "sub_df_list =[]\r\n",
    "for param in parameters:\r\n",
    "    # get a sub_df\r\n",
    "    sub_df = weather_dropped_df[weather_dropped_df['parameter']==param]\r\n",
    "    # reset the index, then access the parameter name in the sub\r\n",
    "    sub_df.reset_index(inplace=True,drop=True)\r\n",
    "    param = sub_df.parameter[0]\r\n",
    "    # rename the value column with a more describing name, drop the parameter column\r\n",
    "    sub_df.columns = ['date',param,param+' value','id_tuple']\r\n",
    "    sub_df = sub_df[['date', 'id_tuple',param+' value']]\r\n",
    "    # append the sub to the sub list\r\n",
    "    sub_df_list.append(sub_df)\r\n",
    "# merge all sub_dfs to create the reformated df\r\n",
    "weather_reformatted_df = sub_df_list[0]\r\n",
    "for sub in sub_df_list[1:]:\r\n",
    "    weather_reformatted_df = pd.merge(weather_reformatted_df, sub, on=['id_tuple','date'],suffixes=('',''))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# simplify the col names of weather_reformatted_df\r\n",
    "# take the col names, create empty list for new names\r\n",
    "cols = weather_reformatted_df.columns\r\n",
    "new_cols = []\r\n",
    "# iterate over the vol names\r\n",
    "for name in cols:\r\n",
    "    pos = name.find(':')\r\n",
    "    # check if there are units given, if yes drop them\r\n",
    "    if pos >0:\r\n",
    "        name = name[:pos]\r\n",
    "    # collect new col names\r\n",
    "    new_cols.append(name)\r\n",
    "# rename the cols of weather_reformatted_df\r\n",
    "weather_reformatted_df.columns=new_cols\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# get for the tilt & align values\r\n",
    "from d00_utils.tiltalign_adjust import tilt_options,align_options\r\n",
    "from collections import deque\r\n",
    "tilt_opts = tilt_options(power_usage_july_total,power_usage_january_total)\r\n",
    "align_suggs = align_options(consumption_night,consumption_morning,consumption_afternoon,consumption_evening)\r\n",
    "\r\n",
    "\r\n",
    "# loop over allowed ids and tilt, align options and find out, how much daily output after high variance factors is available\r\n",
    "highvar_allids=[]\r\n",
    "\r\n",
    "for id in ids_allowed:\r\n",
    "    test = weather_reformatted_df[weather_reformatted_df.id_tuple== id]\r\n",
    "    # reset the index for better integration\r\n",
    "    test.reset_index(drop=True,inplace=True)\r\n",
    "    # gather and process low variance data for the chosen id\r\n",
    "    # begin with extracting latitude and longitude\r\n",
    "    lat = id[0]\r\n",
    "    lon = id[1]\r\n",
    "\r\n",
    "    # you will need elevation data in the surrounding area\r\n",
    "    from d01_data.get_topo_data import elevation_carpet\r\n",
    "    carpet = elevation_carpet(lat,lon)\r\n",
    "    # extract the necessary info from the carpet\r\n",
    "    from d02_intermediate.create_topo_int import carpet2df, ground_grad_ns,ground_grad_ew,carpet_characterics\r\n",
    "    elevation_df = carpet2df(carpet)\r\n",
    "    characteristics = carpet_characterics(carpet)\r\n",
    "    ns_grad = ground_grad_ns(elevation_df,characteristics['ns_stepsize'])\r\n",
    "    ew_grad = ground_grad_ns(elevation_df,characteristics['ew_stepsize'])\r\n",
    "    \r\n",
    "    # get the tilt suggestions\r\n",
    "    natural_tilt = math.degrees(math.atan(ns_grad))\r\n",
    "    tilt_suggs = [natural_tilt] + tilt_opts\r\n",
    "    \r\n",
    "    # save the datetime info in a more fitting column\r\n",
    "    test['datetime'] = test['date']\r\n",
    "    # drop the time info drom the date column\r\n",
    "    test['date'] = test.apply(lambda x: x.date.date(),axis=1)\r\n",
    "\r\n",
    "    # add some geometric values\r\n",
    "    from d02_intermediate.geometry3d import sun_geo as sg\r\n",
    "    # get info about sunheight and azimut for each row\r\n",
    "    test['sungeo'] = test.apply(lambda x: sg(lat,lon,x.datetime),axis=1)\r\n",
    "    test['azimut'] = test.apply(lambda x: x.sungeo.get('azimut'),axis=1)\r\n",
    "    test['sunheight'] = test.apply(lambda x: x.sungeo.get('sunheight refracted'),axis=1)\r\n",
    "    test.drop('sungeo',inplace=True,axis=1)\r\n",
    "    # get data about the albedo\r\n",
    "    test.loc[:,'albedo'] = test.apply(lambda x: albedo(lat,lon),axis=1)\r\n",
    "\r\n",
    "    # ADD IN ITERATION FOR TILTALIGN OPTIONS HERE\r\n",
    "\r\n",
    "    for alignment in align_suggs:\r\n",
    "        for tilt in tilt_suggs:\r\n",
    "\r\n",
    "            # time for high variance factors which vary for each datetime\r\n",
    "            # all this data will be put into additional rows\r\n",
    "\r\n",
    "            # compute the gradients of the panel\r\n",
    "            ns_grad = math.tan(math.radians(tilt))\r\n",
    "            ew_grad = math.tan(math.radians(alignment))\r\n",
    "\r\n",
    "            # get data about the angle of incidence\r\n",
    "            test['angle'] = test.apply(lambda x: inc_angle(x.azimut,x.sunheight,ns_grad,ew_grad),axis=1)\r\n",
    "\r\n",
    "            # iterate over the unique dates, get irradiance_toa info for each date and save in a dict\r\n",
    "            toa_dict = {}\r\n",
    "            for date in test.date.unique():\r\n",
    "                toa_dict[date]= get_solar_irradiance(date)\r\n",
    "            # add the total solar irradiance to the dataframe\r\n",
    "            test['solar_irradiance'] = test.apply(lambda x: toa_dict[x.date],axis=1)\r\n",
    "\r\n",
    "            # compute the incidence on the panel\r\n",
    "            test['incidence'] = test.apply(lambda x: incidence_on_panel(x.global_rad,x.direct_rad,x.diffuse_rad,x.solar_irradiance,x.sunheight,tilt,x.albedo,x.angle),axis=1)\r\n",
    "\r\n",
    "\r\n",
    "            # get the high variance losses\r\n",
    "            # import the needed packages\r\n",
    "            from d02_intermediate.weather_int import est_snowdepth_panel, est_paneltemp, loss_by_snow, loss_by_temp, est_soiling_loss_data\r\n",
    "            # begin with the snow\r\n",
    "            # estimate the snow depth on the panel\r\n",
    "            test['snow_depth_panel'] = test.apply(lambda x: est_snowdepth_panel(x['snow_depth'],tilt),axis=1)\r\n",
    "            # compute the generated losses\r\n",
    "            test['loss_snow'] = test.apply(lambda x: loss_by_snow(x.snow_depth_panel),axis=1)\r\n",
    "\r\n",
    "            # loss by panel temp\r\n",
    "            # estimate the panel temp\r\n",
    "            test['panel_temp'] = test.apply(lambda x: est_paneltemp(x.t_2m,x.wind_speed_10m),axis=1)\r\n",
    "            # estimate the resulting loss in output\r\n",
    "            test['loss_temp']  = test.apply(lambda x: loss_by_temp(x.panel_temp,temp_coeff),axis=1)\r\n",
    "\r\n",
    "            # loss by soiling\r\n",
    "            # create a soil_df with soiling data\r\n",
    "            soil_df = est_soiling_loss_data(natural_tilt, test,'total_aod_550nm','precip_1h','datetime',24)\r\n",
    "            # add the soil loss data to the full dataframe\r\n",
    "            test[['hrs_since_rain','aod_mean','loss_soil']] = soil_df[['hrs_since_rain','aod_mean','soil loss pct']]\r\n",
    "            # all high variance factors are included\r\n",
    "\r\n",
    "            # compute the total loss by those factors and the output after those factors\r\n",
    "            test['loss_highvar'] = test.apply(lambda x: 1-(1-x.loss_snow)*(1-x.loss_temp)*(1-x.loss_soil),axis=1)\r\n",
    "            test['output_highvar'] = test.apply(lambda x: x.incidence*(1-x.loss_highvar),axis=1)\r\n",
    "\r\n",
    "\r\n",
    "            # get the total output of the location per day\r\n",
    "            # compute the covered timespan\r\n",
    "            tdelta = test.datetime.iloc[-1]-test.datetime.iloc[0]\r\n",
    "            timespan = tdelta.days+tdelta.seconds/(24*3600)\r\n",
    "            # get the total output after high variance loss factors\r\n",
    "            output_highvar_total = test.output_highvar.sum()\r\n",
    "            # compute the mean output per day\r\n",
    "            output_highvar_daily = output_highvar_total/timespan\r\n",
    "\r\n",
    "            # collect the most relevant data in a dict\r\n",
    "            relevant = ['loss_temp','loss_snow','loss_soil','loss_highvar']\r\n",
    "            highvar_summary_dict = {}\r\n",
    "            for item in relevant:\r\n",
    "                highvar_summary_dict[item] = test[item].mean()\r\n",
    "\r\n",
    "            # collect the data\r\n",
    "            # relevant: loss factors, loss total, output daily +  tilt, align, id\r\n",
    "            highvar_list = [id,\r\n",
    "                            tilt,\r\n",
    "                            alignment,\r\n",
    "                            output_highvar_daily,\r\n",
    "                            test.loss_highvar.mean(),\r\n",
    "                            test.loss_temp.mean(),\r\n",
    "                            test.loss_snow.mean(),\r\n",
    "                            test.loss_soil.mean(),\r\n",
    "                            test.angle.min()]\r\n",
    "            highvar_allids.append(highvar_list)\r\n",
    "\r\n",
    "# create a new dataframe from all the data\r\n",
    "highvar_allids_df = pd.DataFrame(highvar_allids)\r\n",
    "# name the cols\r\n",
    "highvar_allids_df.columns=['id_tuple','tilt','alignment','output_highvar_daily','loss_highvar','loss_temp','loss_snow','loss_soil','angle']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97.634026610204 77.46623499996447\n",
      "98.8349747253786 77.46623499996447\n",
      "98.99410498500431 77.46623499996447\n",
      "107.5936311307802 77.46623499996447\n",
      "108.31345621182825 77.46623499996447\n",
      "108.21887114084582 77.46623499996447\n",
      "117.54847594992621 77.46623499996447\n",
      "117.95034522097986 77.46623499996447\n",
      "117.68986597934027 77.46623499996447\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "id_loc =2\r\n",
    "subset = highvar_allids_df[highvar_allids_df['id_tuple'] == ids_allowed.iloc[id_loc]]\r\n",
    "max_daily = subset.output_highvar_daily.max()\r\n",
    "subset[subset.output_highvar_daily>.9*max_daily]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test[['datetime','id_tuple','loss_snow','loss_soil','loss_temp','loss_highvar','incidence','output_highvar']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# turn weather dropped into a gdf with point geometries\r\n",
    "gdf = weather_dropped_df\r\n",
    "gdf['geometry'] = gdf.apply(lambda x: Point(x['id_tuple']),axis=1)\r\n",
    "crs = 'epsg:4326'\r\n",
    "gdf = gpd.GeoDataFrame(gdf,crs=crs,geometry=gdf.geometry)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this cell contains outdated code for documentation purposes\r\n",
    "\r\n",
    "# force_polyrequest = True\r\n",
    "# # setting if a new grid will be requested\r\n",
    "# force_new_grid = False\r\n",
    "# # check the number of unique locations\r\n",
    "# # if the number of unique locations is too high for a relatively quick reverse geolocator search, \r\n",
    "# # use a smaller resolution and create polygons for each location\r\n",
    "# if len(weather_dropped_df.id_tuple.unique()) >2000 or force_polyrequest:\r\n",
    "#     # set path to load/save the grid\r\n",
    "#     path = '../data/01_raw/address_grid_'+try_id+'.pkl'\r\n",
    "#     if os.path.exists(path) and not force_new_grid:\r\n",
    "#         # load the grid if possible\r\n",
    "#         with open(path,'rb') as f:\r\n",
    "#             grid_df=pickle.load(f)\r\n",
    "#     else:\r\n",
    "#         # request a grid if none is available\r\n",
    "#         # reference the loc dict to get necessary coordinates\r\n",
    "#         #...\r\n",
    "#         lon_min = 9.8767193\r\n",
    "#         lon_max = 12.6539178\r\n",
    "#         lat_min = 50.2043467\r\n",
    "#         lat_max = 51.6492842\r\n",
    "#         reso = 24\r\n",
    "#         # request the address grid with polygons\r\n",
    "#         grid_df = address.get_address_grid(lat_min,lat_max,lon_min,lon_max,reso)\r\n",
    "#         # save the grid for quicker access\r\n",
    "#         with open(path,'wb') as f:\r\n",
    "#             pickle.dump(grid_df,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this cell contains outdated code, only for documentation purposes\r\n",
    "\r\n",
    "# # load or request address data\r\n",
    "# # check if address data for the current round exists\r\n",
    "# path = '../data/01_raw/address_data_raw' +'_' + try_id +'.csv'\r\n",
    "# if os.path.exists(path) and use_available:\r\n",
    "#     # if available, read currently used address data\r\n",
    "#     address_df = pd.read_csv(path)\r\n",
    "#     # turn id_tuple col into tuples\r\n",
    "#     tups = address_df['id_tuple']\r\n",
    "#     tups = tups.apply(lambda x: make_tuple(x))\r\n",
    "#     address_df['id_tuple'] = tups\r\n",
    "# else:\r\n",
    "#     # if address data isn't available locally, request it from the api\r\n",
    "#     address_df = address.get_address_data(ids_allowed)\r\n",
    "#     # delete ids from ids_all_df if not in address_df\r\n",
    "#     ids_all_df = ids_all_df[ids_all_df['id_tuple'].isin(address_df['id_tuple'])]\r\n",
    "#     # also save it to reduce number of requests\r\n",
    "#     address_df.to_csv(path, index=False)\r\n",
    "\r\n",
    "# # block locations not in germany\r\n",
    "# block_df=idmanage.update_id_block_df(\r\n",
    "#     address_df,\r\n",
    "#     block_df,\r\n",
    "#     'different from',\r\n",
    "#     'de',\r\n",
    "#     block_reason='not in germany',\r\n",
    "#     insights=False,\r\n",
    "#     block_col='country code'\r\n",
    "# )\r\n",
    "# # update ids_all_df and ids_allowed\r\n",
    "# ids_all_df = idmanage.updateblocks_idmanage_df(block_df, ids_all_df)\r\n",
    "# ids_allowed = ids_all_df[ids_all_df['blocked']==False]['id_tuple']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load or request elevation carpets\r\n",
    "path = '../data/01_raw/carpet_pickled' +'_' + try_id +'.pkl'\r\n",
    "# check if data is available\r\n",
    "if os.path.exists(path) and use_available:\r\n",
    "    # load carpet_list if possible\r\n",
    "    with open(path,'rb') as f:\r\n",
    "        carpet_list = pickle.load(f)\r\n",
    "else:\r\n",
    "    # else request and save carpet_list\r\n",
    "    # list of carpet data for each allowed location: carpet_list\r\n",
    "    carpet_list = []\r\n",
    "    # request carpets\r\n",
    "    for tup in ids_allowed:\r\n",
    "        lat = tup[0]\r\n",
    "        lon = tup[1]\r\n",
    "        carpet_list.append(topo_raw.elevation_carpet(lat,lon))\r\n",
    "    # save carpet_list in .pkl file\r\n",
    "    with open(path,'wb') as f:\r\n",
    "        pickle.dump(carpet_list,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# turn all elevation carpets into elevation dataframes, save in df: elevation_df_list\r\n",
    "# also get characteristics for each carpet, save in list: characteristics_list\r\n",
    "elevation_df_list =[]\r\n",
    "characteristics_list =[]\r\n",
    "for carpet in carpet_list:\r\n",
    "    elevation_df = topo_int.carpet2df(carpet)\r\n",
    "    characteristics = topo_int.carpet_characterics(carpet)\r\n",
    "    elevation_df_list.append(elevation_df)\r\n",
    "    characteristics_list.append(characteristics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weather_reformatted_df['total_aod_550nm:idx value'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the ns and ew gradients for each location (facing north), save in lists: ns_grad_list, ew_grad_list\r\n",
    "# get standard deviation of elevation around each location, save in list: elevation_std_list\r\n",
    "ns_grad_list = []\r\n",
    "ew_grad_list = []\r\n",
    "elevation_std_list = []\r\n",
    "for i in range(len(elevation_df_list)):\r\n",
    "    ns_grad_list.append(topo_int.ground_grad_ns(elevation_df_list[i],characteristics_list[i]['ns_stepsize']))\r\n",
    "    ew_grad_list.append(topo_int.ground_grad_ew(elevation_df_list[i],characteristics_list[i]['ew_stepsize']))\r\n",
    "    elevation_std_list.append(topo_int.elevation_std(elevation_df_list[i]))\r\n",
    "# collect intermediate (preprocessed) carpet data in dataframe\r\n",
    "carpet_int = {'ns grad':ns_grad_list,'ew grad':ew_grad_list,'elevation std':elevation_std_list,'id_tuple':ids_allowed}\r\n",
    "carpet_data_int_df = pd.DataFrame(carpet_int)\r\n",
    "carpet_data_int_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# update blocked ids based on the value of the ns_gradient\r\n",
    "block_df = idmanage.update_id_block_df(\r\n",
    "    carpet_data_int_df,\r\n",
    "    block_df,\r\n",
    "    'less than',\r\n",
    "    -0.09,\r\n",
    "    'steiler Hang Richtung Norden',\r\n",
    "    block_col='ns grad'\r\n",
    ")\r\n",
    "# update ids_all_df and ids_allowed\r\n",
    "ids_all_df = idmanage.updateblocks_idmanage_df(block_df, ids_all_df).reset_index(drop=True)\r\n",
    "ids_allowed = ids_all_df[ids_all_df['blocked']==False]['id_tuple'].reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get intermediate topo data that is not blocked\r\n",
    "carpet_data_int_df=carpet_data_int_df[carpet_data_int_df['id_tuple'].isin(block_df['id_tuple'])==False].reset_index(drop=True)\r\n",
    "# add cols for tilt and alignment, calculate the values\r\n",
    "carpet_data_int_df['tilt'] = carpet_data_int_df.apply(lambda x: math.degrees(math.tan(x['ns grad'])), axis=1)\r\n",
    "carpet_data_int_df['alignment'] = carpet_data_int_df.apply(lambda x: topo_int.grads_to_alignment(x['ns grad'],x['ew grad']), axis=1)\r\n",
    "carpet_data_int_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get or load raw elevation_path data looking east and west for each location, also get distance between points on path in meters\r\n",
    "path = '../data/01_raw/ele_path_eastwest_pickled' +'_' + try_id +'.pkl'\r\n",
    "if os.path.exists(path) and use_available:\r\n",
    "    # load elevation path data if available locally\r\n",
    "        with open(path,'rb') as f:\r\n",
    "            ele_path_list_dict = pickle.load(f)\r\n",
    "            # load path lists if available\r\n",
    "            ele_path_east_list = ele_path_list_dict.get('east',math.nan)\r\n",
    "            ele_path_west_list = ele_path_list_dict.get('west',math.nan)\r\n",
    "            step_size_list = ele_path_list_dict.get('stepsizes',math.nan)\r\n",
    "else:\r\n",
    "    # request and save elevation path data if unavailable\r\n",
    "    # create empty lists: ele_path_east_list, ele_path_west_list\r\n",
    "    ele_path_east_list = []\r\n",
    "    ele_path_west_list = []\r\n",
    "    # empty list to save stepsizes between points on path in meter: step_size_list\r\n",
    "    step_size_list = []\r\n",
    "    for tup in ids_allowed:\r\n",
    "        # request all elevation paths for allowed locations\r\n",
    "        lat = tup[0]\r\n",
    "        lon = tup[1]\r\n",
    "        # get the path profile (only elevation values)\r\n",
    "        ele_path_east = topo_raw.elevation_path(lat,lon,direction='east').get('profile')\r\n",
    "        ele_path_west = topo_raw.elevation_path(lat,lon,direction='west').get('profile')\r\n",
    "        # compute stepsize\r\n",
    "        ew_stepsize = math.cos(lat)*111.325\r\n",
    "        # all elevation path to list of elevation paths\r\n",
    "        ele_path_east_list.append(ele_path_east)\r\n",
    "        ele_path_west_list.append(ele_path_west)\r\n",
    "        step_size_list.append(ew_stepsize)\r\n",
    "    ele_path_list_dict = {'east':ele_path_east_list, 'west':ele_path_west_list,'stepsizes':step_size_list}\r\n",
    "    # save pickled elevation path data\r\n",
    "    with open(path,'wb') as f:\r\n",
    "        pickle.dump(ele_path_list_dict,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the highest available gradient from starting point to another point on the elevation path\r\n",
    "\r\n",
    "# empty lists to store highest gradients\r\n",
    "max_grad_east_list, max_grad_west_list = [], []\r\n",
    "# iterate over all paths to find the highest gradient in each direction\r\n",
    "# can be improved by including a variable list of path lists, eg by iterating over the keys of a path_list_dict\r\n",
    "# not needed since there are only to directions of interest\r\n",
    "for i in range(len(ele_path_east_list)):\r\n",
    "    # get the paths, stepsize (distance between path points in m)\r\n",
    "    path_east = ele_path_east_list[i]\r\n",
    "    path_west = ele_path_west_list[i]\r\n",
    "    stepsize = step_size_list[i]\r\n",
    "    # get the max gradient in each path\r\n",
    "    max_grad_east = topo_int.maxgrad_inpath(path_east,stepsize)\r\n",
    "    max_grad_west = topo_int.maxgrad_inpath(path_west,stepsize)\r\n",
    "    # append the max gradient to the corresponding list\r\n",
    "    max_grad_east_list.append(max_grad_east)\r\n",
    "    max_grad_west_list.append(max_grad_west)\r\n",
    "# save the max gradient data in dataframe: max_grad_df\r\n",
    "# use max_grad_dict for easier creation of the df\r\n",
    "max_grad_dict = {'max grad west':max_grad_west_list, 'max grad east':max_grad_east_list}\r\n",
    "max_grad_df = pd.DataFrame.from_dict(max_grad_dict).set_index(ids_allowed,drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# turn the gradients into degrees\r\n",
    "max_grad_df['max west degrees'] = max_grad_df.apply(lambda x: math.atan(x['max grad west'])*180/math.pi, axis=1)\r\n",
    "max_grad_df['max east degrees'] = max_grad_df.apply(lambda x: math.atan(x['max grad east'])*180/math.pi, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the sunheights for different times at the locations\r\n",
    "# trying to find data around sunrise and sunset times\r\n",
    "# save results in dataframe\r\n",
    "# set parameters for sunrise\r\n",
    "starttime = dt.datetime(2000,1,1,hour=4,minute=45)\r\n",
    "minute_delta = dt.timedelta(seconds=300)\r\n",
    "n_times = 12\r\n",
    "date_str = '2021-09-20 '\r\n",
    "\r\n",
    "# create empty lists for sunrises and corresponding times\r\n",
    "sunrise_list, idx_times = [], []\r\n",
    "# iterate over requested times\r\n",
    "for i in range(n_times):\r\n",
    "    # iterate over location tuples, get sunrises for each\r\n",
    "    for tup in max_grad_df.index:\r\n",
    "        lat = tup[0]\r\n",
    "        lon = tup[1]\r\n",
    "        # generate datetime_str for the sh.sunrise function\r\n",
    "        time_str = dt.datetime.strftime(starttime + i*minute_delta,'%H:%M')\r\n",
    "        datetime_str = date_str + time_str\r\n",
    "        # compute sunrise for given time and location\r\n",
    "        sunrise = sh.sunheight(lat,lon,datetime_str)\r\n",
    "        # save the sunrise and time string\r\n",
    "        sunrise_list.append(sunrise)\r\n",
    "    idx_times.append(time_str)\r\n",
    "# reshape sunrise list, put into dataframe, use times for columns\r\n",
    "\r\n",
    "# reshape the sunrise list so it's one col per location\r\n",
    "array = np.array(sunrise_list).reshape(n_times,-1)\r\n",
    "# turn the array into a dataframe, set fitting index and col names\r\n",
    "sunrise_df = pd.DataFrame(array,index=idx_times)\r\n",
    "sunrise_df.columns = max_grad_df.index\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# set parameters for sunset\r\n",
    "starttime = dt.datetime(2000,1,1,hour=16,minute=45)\r\n",
    "minute_delta = dt.timedelta(seconds=300)\r\n",
    "n_times = 12\r\n",
    "date_str = '2021-09-20 '\r\n",
    "\r\n",
    "# create empty lists for sunrises and corresponding times\r\n",
    "sunset_list, idx_times = [], []\r\n",
    "# iterate over requested times\r\n",
    "for i in range(n_times):\r\n",
    "    # iterate over location tuples, get sunrises for each\r\n",
    "    for tup in max_grad_df.index:\r\n",
    "        lat = tup[0]\r\n",
    "        lon = tup[1]\r\n",
    "        # generate datetime_str for the sh.sunrise function\r\n",
    "        time_str = dt.datetime.strftime(starttime + i*minute_delta,'%H:%M')\r\n",
    "        datetime_str = date_str + time_str\r\n",
    "        # compute sunrise for given time and location\r\n",
    "        sunset = sh.sunheight(lat,lon,datetime_str)\r\n",
    "        # save the sunrise and time string\r\n",
    "        sunset_list.append(sunset)\r\n",
    "    idx_times.append(time_str)\r\n",
    "# reshape sunrise list, put into dataframe, use times for columns\r\n",
    "\r\n",
    "# reshape the sunrise list so it's one col per location\r\n",
    "array = np.array(sunset_list).reshape(n_times,-1)\r\n",
    "# turn the array into a dataframe, set fitting index and col names\r\n",
    "sunset_df = pd.DataFrame(array,index=idx_times)\r\n",
    "sunset_df.columns = max_grad_df.index\r\n",
    "# sort sunset_df with ascending degree values\r\n",
    "sunset_df.sort_values(by=sunset_df.columns[0],inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# empty lists to save effective and real sunrise and sunset\r\n",
    "sr_eff_list, sr_real_list, ss_eff_list, ss_real_list = [],[], [], []\r\n",
    "\r\n",
    "# iterate over locations in max_grad_df\r\n",
    "# get real and effective sunrise\r\n",
    "for tup in max_grad_df.index:\r\n",
    "    \r\n",
    "    # if there's shaded time after sunrise (if the degrees value is positive) compute both real and effective sunrise time\r\n",
    "    if max_grad_df['max east degrees'][tup] >0:\r\n",
    "        # get the sunrise times\r\n",
    "        sr_eff_pos = bisect.bisect(sunrise_df[tup],max_grad_df['max east degrees'][tup])\r\n",
    "        sr_real_pos = bisect.bisect(sunrise_df[tup],0)\r\n",
    "        sr_eff = sunrise_df.index[sr_eff_pos]\r\n",
    "        sr_real = sunrise_df.index[sr_real_pos]\r\n",
    "        # save the sunrise times\r\n",
    "        sr_eff_list.append(sr_eff)\r\n",
    "        sr_real_list.append(sr_real)\r\n",
    "    else:\r\n",
    "        # if there's no difference between real and effective sunrise, save the real sunrise for both\r\n",
    "        sr_real_pos = bisect.bisect(sunrise_df[tup],0)\r\n",
    "        sr_real = sunrise_df.index[sr_real_pos]\r\n",
    "        sr_real_list.append(sr_real)\r\n",
    "        sr_eff_list.append(sr_real)\r\n",
    "    # if there's shaded time just before sunset (if the degrees value is positive) compute both real and effective sunset time\r\n",
    "    if max_grad_df['max west degrees'][tup] >0:\r\n",
    "        # get the sunset times\r\n",
    "        ss_eff_pos = bisect.bisect(sunset_df[tup],max_grad_df['max west degrees'][tup])\r\n",
    "        ss_real_pos = bisect.bisect(sunset_df[tup],0)\r\n",
    "        ss_eff = sunset_df.index[ss_eff_pos]\r\n",
    "        ss_real = sunset_df.index[ss_real_pos]\r\n",
    "        # save the sunrise times\r\n",
    "        ss_eff_list.append(ss_eff)\r\n",
    "        ss_real_list.append(ss_real)\r\n",
    "    else:\r\n",
    "        # if there's no difference between real and effective sunset, save the real sunset for both\r\n",
    "        ss_real_pos = bisect.bisect(sunset_df[tup],0)\r\n",
    "        ss_real = sunset_df.index[ss_real_pos]\r\n",
    "        ss_real_list.append(ss_real)\r\n",
    "        ss_eff_list.append(ss_real)\r\n",
    "\r\n",
    "\r\n",
    "# save sunrise data in dataframe: sunrise_sunset_timeloss_df\r\n",
    "sunrise_sunset_timeloss_df = pd.DataFrame()\r\n",
    "sunrise_sunset_timeloss_df.index = max_grad_df.index\r\n",
    "sunrise_sunset_timeloss_df['real sunrise'] = sr_real_list\r\n",
    "sunrise_sunset_timeloss_df['effective sunrise'] = sr_eff_list\r\n",
    "sunrise_sunset_timeloss_df['real sunset'] = ss_real_list\r\n",
    "sunrise_sunset_timeloss_df['effective sunset'] = ss_eff_list\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sunrise_sunset_timeloss_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "to_datetime = lambda x: dt.datetime.strptime(x,'%H:%M')\r\n",
    "# compute time lost by shade just after sunrise and just before sunset\r\n",
    "timeloss_help_df = sunrise_sunset_timeloss_df.applymap(to_datetime)\r\n",
    "# calculate day lengths\r\n",
    "timeloss_help_df['real day length'] = timeloss_help_df.apply(lambda x: x['real sunset']-x['real sunrise'], axis=1)\r\n",
    "timeloss_help_df['effective day length'] = timeloss_help_df.apply(lambda x: x['effective sunset']-x['effective sunrise'], axis=1)\r\n",
    "# calculate absolute and relative losses of sun minutes by shade thrown by nearby elevations\r\n",
    "timeloss_help_df['abs time loss by elevationshade'] = timeloss_help_df.apply(lambda x: x[4]-x[5], axis=1)\r\n",
    "timeloss_help_df['rel time loss by elevationshade'] = timeloss_help_df.apply(lambda x: x[6]/x[4], axis=1)\r\n",
    "# add the loss of sun minutes to sunrise_sunset_timeloss_df\r\n",
    "sunrise_sunset_timeloss_df[['abs time loss by elevationshade','rel time loss by elevationshade']] = timeloss_help_df[['abs time loss by elevationshade','rel time loss by elevationshade']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get reduced df with id and loss pct: elevationshade_red_df\r\n",
    "elevationshade_red_df= sunrise_sunset_timeloss_df[['rel time loss by elevationshade']]\r\n",
    "elevationshade_red_df.columns = ['loss by elevationshade']\r\n",
    "elevationshade_red_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# block ids based on max gradients if necessary, update id management dfs, save updated id_all_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get processed topo data with expected losses\r\n",
    "# make sure only allowed locations are processed\r\n",
    "carpet_data_int_df=carpet_data_int_df[carpet_data_int_df['id_tuple'].isin(block_df['id_tuple'])==False].reset_index(drop=True)\r\n",
    "# get expected losses by tilt and alignment\r\n",
    "# load the efficiency dataframe\r\n",
    "path = '../references/Efficiency of solar panels by tilt and alignment in germany.xlsx'\r\n",
    "efficiency_df = pd.read_excel(path, index_col=0)\r\n",
    "# calculate the losses and save in topo_loss_tiltalign_df\r\n",
    "topo_loss_tiltalign_df = pd.DataFrame()\r\n",
    "topo_loss_tiltalign_df['id_tuple'] = carpet_data_int_df['id_tuple']\r\n",
    "topo_loss_tiltalign_df['loss by tilt, alignment'] = carpet_data_int_df.apply(lambda x: topo_pro.tiltalign_loss_lowres(x.tilt,x.alignment,efficiency_df),axis=1)\r\n",
    "topo_loss_tiltalign_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create geodataframe with all allowed locations, all losses and geometries for each position\r\n",
    "\r\n",
    "# instantiate loss_df with all losses\r\n",
    "loss_df = topo_loss_tiltalign_df\r\n",
    "if 'id_tuple' in loss_df.columns:\r\n",
    "    loss_df.set_index('id_tuple',drop=True,inplace=True)\r\n",
    "# join loss by elevationshade\r\n",
    "loss_df = loss_df.join(elevationshade_red_df)\r\n",
    "# calculate total loss percentage\r\n",
    "loss_df['total losses'] = loss_df.apply(lambda x: 1-(1-x[0])*(1-x[1]),axis=1)\r\n",
    "# turn loss_df into geodataframe with polygons\r\n",
    "# get polygons first\r\n",
    "loss_gdf = creategeo.create_polygons(loss_df)\r\n",
    "# change type to gdf\r\n",
    "crs = 'epsg:4326'\r\n",
    "loss_gdf = gpd.GeoDataFrame(loss_gdf, crs=crs, geometry=loss_gdf.geometry)\r\n",
    "loss_gdf"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mapenv': conda)"
  },
  "interpreter": {
   "hash": "c7493435b741875e47357b1d4e2959b9ee7bbd8a014d8aa9e12beae2b4f843e2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}