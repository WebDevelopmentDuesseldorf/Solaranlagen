{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 145,
            "source": [
                "# imports\r\n",
                "import pandas as pd\r\n",
                "import json\r\n",
                "import os\r\n",
                "from IPython.core.debugger import set_trace\r\n",
                "# change directory\r\n",
                "# os.chdir(os.path.dirname(os.getcwd()))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 146,
            "source": [
                "os.chdir('c:\\\\Users\\\\Techie\\\\Documents\\\\soenke\\\\Solaranlagen\\\\src')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 147,
            "source": [
                "# settings for Lottie (Animations)\r\n",
                "\r\n",
                "data_points_url = 'https://assets5.lottiefiles.com/packages/lf20_6dboqita.json'\r\n",
                "wondering_url = 'https://assets4.lottiefiles.com/packages/lf20_pcoatxlk.json'\r\n",
                "typing_url = 'https://assets1.lottiefiles.com/temporary_files/r5WAZZ.json'\r\n",
                "reportlist_url='https://assets9.lottiefiles.com/packages/lf20_dq6whaxi.json'\r\n",
                "options = dict(loop=True, autoplay=True, rendererSettings=dict(preserveAspectRatio='xMidYMid slice'))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 148,
            "source": [
                "# load settings\r\n",
                "with open('../references/settings/user_settings.json','r') as f:\r\n",
                "    settings = json.load(f)\r\n",
                "with open('../references/settings/extended_settings.json','r') as f:\r\n",
                "    settings.update(json.load(f))\r\n",
                "with open('../references/settings/dev_settings.json','r') as f:\r\n",
                "    settings.update(json.load(f))\r\n",
                "\r\n",
                "# compute the real compensation per kWh fed into the grid in 0,01*€\r\n",
                "scale = settings['project_info']['scale']\r\n",
                "compensation = (\r\n",
                "    settings['financial']['base_compensation'][scale]\r\n",
                "    *0.985**(pd.to_datetime(settings['project_info']['planned_commission_date']).month-9)\r\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 149,
            "source": [
                "# set application title\r\n",
                "application_name = 'BraSolar'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 150,
            "source": [
                "# import libraries & components\r\n",
                "\r\n",
                "# import custom components\r\n",
                "# in the future the source will need to be changed (add d07_visualisation in front of 'components')\r\n",
                "from components import start_button, no_fig, your_fig, description\r\n",
                "\r\n",
                "# \r\n",
                "import plotly.express as px\r\n",
                "from jupyter_dash import JupyterDash\r\n",
                "import dash_core_components as dcc\r\n",
                "import dash_html_components as html\r\n",
                "from dash.dependencies import Input, Output, State\r\n",
                "import dash_bootstrap_components as dbc \r\n",
                "from dash_extensions import Lottie\r\n",
                "import pandas as pd\r\n",
                "import os\r\n",
                "from dash import no_update\r\n",
                "from dash.exceptions import PreventUpdate\r\n",
                "from time import sleep, perf_counter\r\n",
                "import geopandas as gpd\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 151,
            "source": [
                "# Bootstrap themes by Ann: https://hellodash.pythonanywhere.com/theme_explorer\r\n",
                "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.SANDSTONE])\r\n",
                "app.title = application_name\r\n",
                "\r\n",
                "app.layout = dbc.Container([\r\n",
                "    # row one\r\n",
                "    dbc.Row([\r\n",
                "        # row one col one\r\n",
                "        dbc.Col([\r\n",
                "            # logo\r\n",
                "            dbc.Card([\r\n",
                "                dbc.CardImg(src='../assets/logo.png',\r\n",
                "                style={'height':'50px', 'width': '50px'}\r\n",
                "                )\r\n",
                "            ],className='mb-2'),\r\n",
                "            # name and link to website\r\n",
                "            dbc.Card([\r\n",
                "                dbc.CardBody([\r\n",
                "                    dbc.CardLink('Sönke Maibach',target='_blank',  # _blank opens a new tab on click\r\n",
                "                    href='https://mabikono.de')\r\n",
                "                ])\r\n",
                "            ],style={'height':'inherit'}),\r\n",
                "        ], width=2),\r\n",
                "        # row one col two\r\n",
                "        dbc.Col([\r\n",
                "            # title and short info\r\n",
                "            dbc.Card([\r\n",
                "                dbc.CardBody([\r\n",
                "                    html.H4('{}'.format(application_name)),\r\n",
                "                    html.P('Find out if your chosen locations are suitable for solar power.')\r\n",
                "                ])\r\n",
                "            ], color = 'info',style={'height':'18vh'}),\r\n",
                "        ], width=6),\r\n",
                "        # row one col three\r\n",
                "        # Dark/Light Mode and Language Dropdown\r\n",
                "        dbc.Col([\r\n",
                "            dbc.Card([\r\n",
                "                dbc.Button('Turn on light',color='primary', id='theme_switch')\r\n",
                "            ])\r\n",
                "        ],width=2)\r\n",
                "    ], className='mb-2 mr-2 ml-2 h-25'),\r\n",
                "    # row two\r\n",
                "    dbc.Row([\r\n",
                "        # row two col one\r\n",
                "        # description\r\n",
                "        dbc.Col([\r\n",
                "            dbc.Card([\r\n",
                "                dbc.CardHeader(Lottie(options=options,url=wondering_url, width='40%', height='40%')),\r\n",
                "                dbc.CardBody([\r\n",
                "                    html.H4('What is going on here?',className='card-title'),\r\n",
                "                    description\r\n",
                "                ])\r\n",
                "            ]),\r\n",
                "        ], width=4, style={'height':'inherit'}),\r\n",
                "        # row two col two\r\n",
                "        # inputs and start button\r\n",
                "        dbc.Col([\r\n",
                "            dbc.Card([\r\n",
                "                # card header for the inputs\r\n",
                "                dbc.CardHeader(\r\n",
                "                    Lottie(options=options,url=typing_url, height='25%', width='25%')\r\n",
                "                ),\r\n",
                "                dbc.CardBody([\r\n",
                "                    html.H4('Please tell me what interests you'),\r\n",
                "                    # input field for location\r\n",
                "                    html.I(\"Choose a city, region or country.\"),\r\n",
                "                    html.Br(),\r\n",
                "                    dcc.Input(id=\"location_input\", type=\"search\",\r\n",
                "                    value=\"Duesseldorf\",\r\n",
                "                    style={'marginRight':'20px'}),\r\n",
                "                    html.Br(), html.Br(),\r\n",
                "\r\n",
                "                    \r\n",
                "                    \r\n",
                "                    # radioitems resolution\r\n",
                "                    html.I(\"Choose the resolution. A higher resolution means more data points (DP) will be analyzed.\"),\r\n",
                "                    html.Br(),\r\n",
                "                    dcc.RadioItems(id='resolution_radio',\r\n",
                "                        options=[\r\n",
                "                            {'label':' Fastest (~100 DP)','value':200},\r\n",
                "                            {'label':' Balanced (~400 DP)','value':800},\r\n",
                "                            {'label':' Very detailed (~1000 DP)','value':2000},\r\n",
                "                        ],\r\n",
                "                        value=200,\r\n",
                "                        labelStyle={'display': 'block'}\r\n",
                "                    ),\r\n",
                "\r\n",
                "                    # Household Size input\r\n",
                "                    html.I('How many people live in your household? Children count as 0.5.'),\r\n",
                "                    html.Br(),\r\n",
                "                    dcc.Input(id=\"household_input\", type=\"number\",\r\n",
                "                    value=3,\r\n",
                "                    style={'marginRight':'20px'}),\r\n",
                "                    html.Br(), html.Br(),\r\n",
                "\r\n",
                "                    # start button\r\n",
                "                    html.Div('',id='old_request'),\r\n",
                "                    start_button,\r\n",
                "\r\n",
                "                    # choose output to plot\r\n",
                "                    html.I('Choose what you want to see.'), html.Br(),\r\n",
                "                    dcc.RadioItems(\r\n",
                "                        options=[\r\n",
                "                            {'label':'Maximum electricity output','value':'maxout'},\r\n",
                "                            {'label':'Maximum savings/financial return','value':'maxroi'},\r\n",
                "                        ],\r\n",
                "                        value='maxout',\r\n",
                "                        labelStyle={'display':'block'},\r\n",
                "                        id='to_plot_radio'\r\n",
                "                    )\r\n",
                "\r\n",
                "                ])\r\n",
                "            ]),\r\n",
                "        ], width=4, style={'height':'inherit'}),\r\n",
                "        # row two col three\r\n",
                "        # status updates\r\n",
                "        dbc.Col([\r\n",
                "            dbc.Card([\r\n",
                "                dbc.CardHeader(Lottie(url=reportlist_url,options=options,height='20%',width='20%')),\r\n",
                "                dbc.CardBody([\r\n",
                "                    html.H4('Status report'),\r\n",
                "                    html.Div('Status updates will be displayed after starting the analysis.',id='started_status'),\r\n",
                "                    html.Div('',id='loc_ref_status'),\r\n",
                "                    html.Div('',id='get_weather_status'),\r\n",
                "                    html.Div('',id='loc_ref_block_status'),\r\n",
                "                    html.Div('',id='weather_pivot_status'),\r\n",
                "                    html.Div('',id='topo_get_status'),\r\n",
                "                    html.Div('',id='terrain_block_status'),\r\n",
                "                    html.Div('',id='sungeo_status'),\r\n",
                "                    html.Div('',id='pollution_status'),\r\n",
                "                    html.Div('',id='need_status'),\r\n",
                "                    html.Div('',id='angle_status'),\r\n",
                "                    html.Div('',id='output_status'),\r\n",
                "                    html.Div('',id='maxout_status'),\r\n",
                "                    html.Div('',id='maxroi_status'),\r\n",
                "                    html.Div('',id='plotted_status'),\r\n",
                "                ],id='status_card')\r\n",
                "            ]),\r\n",
                "        ], width=4, style={'height':'inherit'}),\r\n",
                "    ],className='m-2 mt-4 mb-4 h-15'),\r\n",
                "    # row three \r\n",
                "    dbc.Row([\r\n",
                "        # row three col one\r\n",
                "        dbc.Col([\r\n",
                "            dbc.Card([\r\n",
                "                dbc.CardBody([\r\n",
                "                    dcc.Graph(id='map_graph',figure=no_fig),\r\n",
                "                ])\r\n",
                "            ]),\r\n",
                "        ], width=12,style={}),\r\n",
                "        # row three col two\r\n",
                "        # additional graph next to map\r\n",
                "\r\n",
                "    ],className='m-2',style={\"height\":\"100%\"}),\r\n",
                "\r\n",
                "    dbc.Row([\r\n",
                "        # hidden data storage\r\n",
                "        # store min and max dates, datetimes\r\n",
                "        dcc.Store(id='minmax_date_store'), \r\n",
                "        dcc.Store(id='minmax_datetime_store'),\r\n",
                "\r\n",
                "        # store the process_id\r\n",
                "        dcc.Store(id='process_id_store',data=['sample_id']),\r\n",
                "    ],className='mt-2 mr-2 ml-2',style={}),\r\n",
                "    \r\n",
                "\r\n",
                "], fluid=True, style={\r\n",
                "    \r\n",
                "    'background-image':'url(\"/assets/bg_solar.jpg\")',\r\n",
                "    'background-repeat':'no-repeat',\r\n",
                "    'background-size':'cover',\r\n",
                "    'background-position': 'bottom-center',\r\n",
                "    'background-attachment':'fixed',\r\n",
                "    'font-family': \"calibri\"\r\n",
                "    })"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Callbacks"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 152,
            "source": [
                "# utility imports\r\n",
                "import os\r\n",
                "from time import sleep, perf_counter\r\n",
                "from ast import literal_eval"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 153,
            "source": [
                "# reset outputs when changing user input\r\n",
                "# check if it's a completely new request and inform the user if that's the case\r\n",
                "@app.callback(\r\n",
                "    Output('start_button','n_clicks'),\r\n",
                "    Output('old_request','children'),\r\n",
                "    Output('process_id_store','data'),\r\n",
                "    Input('location_input','value'),\r\n",
                "    Input('resolution_radio','value'),\r\n",
                "    Input('household_input','value')\r\n",
                ")\r\n",
                "def reset_analysis(loc,res,hh):\r\n",
                "    print('clicks reset')\r\n",
                "\r\n",
                "    # check if the results of the request are available in the database \r\n",
                "    # get the process id\r\n",
                "    process_id = str(loc).lower()+'_'+str(res)\r\n",
                "    # get the save paths\r\n",
                "    max_out_path = '../data/03_processed/max_out_{}.pqt'.format(process_id)\r\n",
                "    max_roi_path = '../data/03_processed/max_roi_{}.pqt'.format(process_id)\r\n",
                "    polygons_path = '../references/polygons_{}.json'.format(process_id)\r\n",
                "\r\n",
                "    if os.path.exists(max_out_path) and os.path.exists(max_roi_path) and os.path.exists(polygons_path):\r\n",
                "        user_info = 'This was requested before. Output should be available quickly.'\r\n",
                "    else:\r\n",
                "        user_info = 'This is a new request. Please be patient.'\r\n",
                "\r\n",
                "    return (\r\n",
                "        # reset n_clicks\r\n",
                "        0,\r\n",
                "        # inform user if a long wait time is to be expected\r\n",
                "        user_info,\r\n",
                "        # set process_id\r\n",
                "        [process_id],\r\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 154,
            "source": [
                "# start button func\r\n",
                "# triggered by clicking the start button\r\n",
                "# outputs text to the 'started_status' wrapper\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('started_status','children'),\r\n",
                "    Input('start_button','n_clicks'),\r\n",
                "    State('old_request','children')\r\n",
                "\r\n",
                ")\r\n",
                "def start_analysis(clicks,req):\r\n",
                "    if clicks == 0:\r\n",
                "        return ('Status updates will be displayed after starting the analysis.')\r\n",
                "    else:\r\n",
                "        return '0% done - Analysis started'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 155,
            "source": [
                "# when the started_status updates, make sure a location reference polygon is available\r\n",
                "# after checking (or requesting) update the corresponding status div\r\n",
                "\r\n",
                "# imports\r\n",
                "from d01_data.get_address_data import load_save_reference as loadsaveref\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('loc_ref_status','children'),\r\n",
                "    Input('started_status','children'),\r\n",
                "    State('location_input','value')\r\n",
                ")\r\n",
                "def check_locref(started,loc):\r\n",
                "    if started[0] == 'S':\r\n",
                "        print('locref triggered but no update')\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        starttime = perf_counter()\r\n",
                "        locref = loadsaveref(loc)\r\n",
                "        print('locref triggered and work done')\r\n",
                "        elapsed = perf_counter()-starttime\r\n",
                "        return ('1% done - Location understood in '+f\" {elapsed:0.2f} seconds.\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 156,
            "source": [
                "# after checking the location reference, check if weather data is available\r\n",
                "# if it's missing, request it\r\n",
                "\r\n",
                "# imports\r\n",
                "from d01_data.get_weather_data import getload_weather\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('get_weather_status','children'),\r\n",
                "    Input('loc_ref_status','children'),\r\n",
                "    State('location_input','value'),\r\n",
                "    State('resolution_radio','value'),\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def get_weather(locref,loc,res,process_id):\r\n",
                "    if locref == '':\r\n",
                "        print('get weather triggered, return empty string')\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        starttime = perf_counter()\r\n",
                "        weather_df = getload_weather(process_id[0],location=str(loc),resolution=res)\r\n",
                "\r\n",
                "        print('weather checked, id: {}'.format(process_id))\r\n",
                "\r\n",
                "        elapsed = perf_counter() - starttime\r\n",
                "        return ('30% done - Weather data loaded in '+f'{elapsed:0.2f} seconds')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 157,
            "source": [
                "# start the id management process\r\n",
                "# create and save an id_management_df with info about allowed and disallowed ids + reasons for blocking\r\n",
                "\r\n",
                "\r\n",
                "# imports\r\n",
                "from d00_utils.id_management import id_manage_df, id_allow_df, update_id_block_df, updateblocks_idmanage_df\r\n",
                "import shapely.geometry as sg\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('loc_ref_block_status','children'),\r\n",
                "    Input('get_weather_status','children'), # this should trigger when the weather status changes,\r\n",
                "    State('location_input','value'),\r\n",
                "    State('resolution_radio','value'),\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def loc_ref_block(prev_status,loc,res,process_id):\r\n",
                "    if prev_status =='':\r\n",
                "        print('loc ref block triggered, nothing returned')\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        starttime = perf_counter()\r\n",
                "\r\n",
                "        process_id = process_id[0]\r\n",
                "        \r\n",
                "        # load the weather df, but only id col\r\n",
                "        ids_df = pd.read_parquet('../data/01_raw/weather_{}.pqt'.format(process_id))['id_tuple'].unique()\r\n",
                "        ids_df = pd.DataFrame(ids_df)\r\n",
                "        ids_df.columns = ['id_tuple']\r\n",
                "        print('ids df is a '+str(type(ids_df)))\r\n",
                "\r\n",
                "        global df\r\n",
                "        df = ids_df\r\n",
                "\r\n",
                "        # create dfs to manage ids and blocks\r\n",
                "        idmanage_df = id_manage_df(ids_df,id_col='id_tuple')\r\n",
                "        idmanage_df.id_tuple = [literal_eval(tup) for tup in idmanage_df.id_tuple]\r\n",
                "        block_df = pd.DataFrame(columns=['id_tuple','blocked','block reason'])\r\n",
                "\r\n",
                "        print('pre point')\r\n",
                "        # create a geometry col\r\n",
                "\r\n",
                "        # update the block_df: block locations outside of the requested region\r\n",
                "        block_df = update_id_block_df(\r\n",
                "            ids_df,block_df,\r\n",
                "            'outside of',\r\n",
                "            loadsaveref(str(loc).lower()),\r\n",
                "            'outside of '+loc,\r\n",
                "            block_col='geometry',\r\n",
                "            insights=False)\r\n",
                "        print('blocks updated')\r\n",
                "        # update the main management df\r\n",
                "        idmanage_df = updateblocks_idmanage_df(block_df, idmanage_df)\r\n",
                "        # change id_tuple dtype to string, then save the main df\r\n",
                "        idmanage_df.id_tuple = idmanage_df.id_tuple.astype(str)\r\n",
                "        idmanage_df.to_parquet('../references/id_management/id_manage_df_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        print('blocks by locref done')\r\n",
                "        elapsed = perf_counter()-starttime\r\n",
                "        return ('32% done - Weather data double-checked in '+f'{elapsed:0.2f} seconds')\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 158,
            "source": [
                "# pivot the weather df\r\n",
                "\r\n",
                "# imports\r\n",
                "import datetime as dt\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('weather_pivot_status','children'),\r\n",
                "    Output('minmax_date_store','data'),\r\n",
                "    Output('minmax_datetime_store','data'),\r\n",
                "    Input('loc_ref_block_status','children'), # trigger when the step before is done\r\n",
                "    State('location_input','value'),\r\n",
                "    State('resolution_radio','value'),\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def weather_pivot(locref_status,loc,res,process_id):\r\n",
                "    if locref_status == '':\r\n",
                "        return (\r\n",
                "            # return empty string for the status div\r\n",
                "            '',\r\n",
                "            # return no update for the minmax date and datetime stores\r\n",
                "            no_update,no_update\r\n",
                "        )\r\n",
                "    else:\r\n",
                "        starttime = perf_counter()\r\n",
                "\r\n",
                "        # recreate process_id\r\n",
                "        process_id = process_id[0]\r\n",
                "        # load weather\r\n",
                "        weather_df =pd.read_parquet('../data/01_raw/weather_{}.pqt'.format(process_id))\r\n",
                "        # load allowed ids from idmanage_df\r\n",
                "        idmanage_df = pd.read_parquet('../references/id_management/id_manage_df_{}.pqt'.format(process_id))\r\n",
                "        idmanage_df = idmanage_df.loc[idmanage_df.blocked ==False].id_tuple\r\n",
                "\r\n",
                "        # filter weather by allowed ids\r\n",
                "        weather_df = weather_df.loc[weather_df.id_tuple.isin(idmanage_df)]\r\n",
                "\r\n",
                "        # pivot the weather df so that all data corresponding to a specific time and location are in a single row\r\n",
                "        weather_df = weather_df.pivot(['id_tuple','date'],'parameter','value').reset_index()\r\n",
                "        # create separate cols for date and datetime\r\n",
                "        weather_df['datetime'] = weather_df.date\r\n",
                "        weather_df['date'] = [x.date() for x in weather_df.date]\r\n",
                "\r\n",
                "        # simplify the col names of weather_df\r\n",
                "        # take the col names, create empty list for new names\r\n",
                "        cols = weather_df.columns\r\n",
                "        new_cols = []\r\n",
                "        # iterate over the vol names\r\n",
                "        for name in cols:\r\n",
                "            pos = name.find(':')\r\n",
                "            # check if there are units given, if yes drop them\r\n",
                "            if pos >0:\r\n",
                "                name = name[:pos]\r\n",
                "            # collect new col names\r\n",
                "            new_cols.append(name)\r\n",
                "        # rename the cols of weather_df\r\n",
                "        weather_df.columns=new_cols\r\n",
                "\r\n",
                "        # new col am which states if a row has a MEST in the morning\r\n",
                "        weather_df.loc[:,'pm'] = [\r\n",
                "            (x.hour>10) & (x.hour<23)\r\n",
                "            for x\r\n",
                "            in weather_df.datetime]\r\n",
                "\r\n",
                "        # get the min and max datetime as well as min and max dates\r\n",
                "        minmax_dates = {'min':weather_df.date.iat[0], 'max':weather_df.date.iat[-1]}\r\n",
                "        minmax_datetimes = {'min':weather_df.datetime.iat[0], 'max':weather_df.datetime.iat[-1]}\r\n",
                "\r\n",
                "\r\n",
                "        # save the pivoted and filtered weather df as weather_df_pivoted\r\n",
                "        weather_df.to_parquet('../data/01_raw/weather_pivoted_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # print update for devs\r\n",
                "        print('weather pivoted')\r\n",
                "\r\n",
                "        # get needed timespan and return update to user\r\n",
                "        elapsed = perf_counter()-starttime\r\n",
                "    \r\n",
                "        return (\r\n",
                "            # return the status update\r\n",
                "            ('38% done - Weather data reformatted in '+f'{elapsed:0.2f} seconds'),\r\n",
                "            # return minmax dates and minmax datetimes\r\n",
                "            minmax_dates, minmax_datetimes\r\n",
                "        )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "source": [
                "# analyse the topographical surroundings\r\n",
                "# make sure topo data is available, if not, request and save it\r\n",
                "# topo data will be saved forever\r\n",
                "# analyse the environment for uneven ground or north facing slopes\r\n",
                "# use the analysis to block locations\r\n",
                "'''this could be improved in the future: better understanding \r\n",
                "    of the specific locations in the weather df could lead to less requested and \r\n",
                "    lower the need for requests\r\n",
                "    also there is room for improvement by putting the async functions into their own module\r\n",
                "    '''\r\n",
                "\r\n",
                "# imports\r\n",
                "from d01_data.get_topo_data import run_requests\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('topo_get_status','children'),\r\n",
                "    Input('weather_pivot_status','children'), # should be triggered when the weather data is pivoted\r\n",
                "    State('location_input','value'),\r\n",
                "    State('resolution_radio','value'),\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def get_topo(weather_status,loc,res,process_id):\r\n",
                "    if weather_status =='':\r\n",
                "        print('no weather data, waiting')\r\n",
                "        return '' # return empty if the last step is not done\r\n",
                "    else:\r\n",
                "        print('timer started for requests')\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter() \r\n",
                "\r\n",
                "        # access process_id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "        # load the idmanage_df, this will be needed later on\r\n",
                "        idmanage_df = pd.read_parquet('../references/id_management/id_manage_df_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # check if topo data relevant to the user request is available\r\n",
                "        topo_path = '../data/02_intermediate/topo_data_' + process_id +'.pqt'\r\n",
                "        if os.path.exists(topo_path):\r\n",
                "            # if the data is available, load it\r\n",
                "            print('topo available')\r\n",
                "        else:\r\n",
                "            # request topo data\r\n",
                "            print('topo requested')\r\n",
                "            # get allowed ids\r\n",
                "            ids_allowed = idmanage_df.loc[idmanage_df.blocked ==False][['id_tuple']]\r\n",
                "            # change dtype to tuple\r\n",
                "            ids_allowed.id_tuple = ([\r\n",
                "                literal_eval(tup)\r\n",
                "                for tup in ids_allowed.id_tuple\r\n",
                "            ])\r\n",
                "            # inform devs of dtype\r\n",
                "            print('thats the type: {}'.format(type(ids_allowed.iat[0,0])))\r\n",
                "            # run the request, this will also save the topo df\r\n",
                "            res = run_requests(ids_allowed.id_tuple,process_id)\r\n",
                "            print('requests finished')\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed = perf_counter()-starttime\r\n",
                "        return ('45% done - Topographical data loaded in '+f'{elapsed:0.2f} seconds')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 160,
            "source": [
                "# block locations based on topographical conditions\r\n",
                "# if there's a north facing slope or uneven ground at the location it will be blocked\r\n",
                "\r\n",
                "# imports\r\n",
                "from d00_utils.id_management import update_id_block_df, updateblocks_idmanage_df\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('terrain_block_status','children'),\r\n",
                "    # triggers after making sure topo data is available\r\n",
                "    Input('topo_get_status','children'),\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def block_by_topo(status,process_id):\r\n",
                "    if status == '':\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter()\r\n",
                "        print('topo block started')\r\n",
                "        # get process id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "        # load topo data and idmanage_df\r\n",
                "        topo_path = '../data/02_intermediate/topo_data_' + process_id +'.pqt'\r\n",
                "        topo_df = pd.read_parquet(topo_path)\r\n",
                "        idmanage_df = pd.read_parquet('../references/id_management/id_manage_df_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # create a block_df as needed by the update_id_block_df function\r\n",
                "        block_df = idmanage_df.loc[idmanage_df.blocked]\r\n",
                "\r\n",
                "        # block locations with uneven ground\r\n",
                "        # if the standard deviation of the elevation is greater than the allowed value, the location will be blocked\r\n",
                "        # if you know what you are doing, you can change the max value by accessing /references/dev_settings.json\r\n",
                "        #   and changing the values in elevation_std_ref\r\n",
                "        \r\n",
                "        # compare the elevation std with the max allowed std based on the project scope\r\n",
                "        # load the reference value first\r\n",
                "        # get the scale from the settings\r\n",
                "        scale = settings['project_info']['scale']\r\n",
                "        # get the reference value from the settings which fits the scale\r\n",
                "        elevation_std_ref = settings['topo_references']['elevation_std'][scale]\r\n",
                "\r\n",
                "        # update the block_df based on the elevation std\r\n",
                "        block_df = update_id_block_df(\r\n",
                "            topo_df,\r\n",
                "            block_df,\r\n",
                "            'greater than',\r\n",
                "            elevation_std_ref,\r\n",
                "            'ground needs to be more even',\r\n",
                "            block_col='elevation_std'\r\n",
                "        )\r\n",
                "\r\n",
                "\r\n",
                "        # compare the slope at a given location with the allowed reference. \r\n",
                "        # if the gradient is less than the relevance, there's a north facing slope. the loc will be blocked\r\n",
                "        # get the reference value from the settings which fits the scale\r\n",
                "        min_ns_grad_ref = settings['topo_references']['min_ns_grad'][scale]\r\n",
                "        # update the block_df \r\n",
                "        # update blocks if the ground is facing north\r\n",
                "        block_df = update_id_block_df(\r\n",
                "            topo_df,\r\n",
                "            block_df,\r\n",
                "            'less than',\r\n",
                "            min_ns_grad_ref,\r\n",
                "            'north facing slope',\r\n",
                "            block_col='ns_gradient'\r\n",
                "        )\r\n",
                "\r\n",
                "        # update the idmanage_df and save it\r\n",
                "        idmanage_df = updateblocks_idmanage_df(block_df, idmanage_df)\r\n",
                "        idmanage_df.to_parquet('../references/id_management/id_manage_df_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed = perf_counter()-starttime\r\n",
                "        return ('53% done - Topographical conditions analyzed in '+f'{elapsed:0.2f} seconds')\r\n",
                "        "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 161,
            "source": [
                "# compute the position of the sun relative to locations\r\n",
                "# also compute the mean aod since the last rain\r\n",
                "\r\n",
                "# imports \r\n",
                "from d02_intermediate.geometry3d import sun_geo\r\n",
                "from d02_intermediate.est_albedo import albedo\r\n",
                "from d02_intermediate.weather_int import rain_prep\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('sungeo_status','children'),\r\n",
                "    Output('pollution_status','children'),\r\n",
                "    Input('terrain_block_status','children'),\r\n",
                "    State('process_id_store','data'),\r\n",
                "    State('minmax_datetime_store','data')\r\n",
                ")\r\n",
                "def sun_geometry(status,process_id,minmax_datetime):\r\n",
                "    if status == '':\r\n",
                "        return '',''\r\n",
                "    else:\r\n",
                "        # start timer\r\n",
                "        starttime = perf_counter()\r\n",
                "\r\n",
                "        # get process_id\r\n",
                "        process_id =process_id[0]\r\n",
                "\r\n",
                "        # load weather_df\r\n",
                "        weather_df = pd.read_parquet('../data/01_raw/weather_pivoted_{}.pqt'.format(process_id))\r\n",
                "        # load idmanage_df\r\n",
                "        idmanage_df = pd.read_parquet('../references/id_management/id_manage_df_{}.pqt'.format(process_id))\r\n",
                "        # filter weather_df by allowed ids\r\n",
                "        weather_df = weather_df.loc[\r\n",
                "            weather_df.id_tuple.isin(\r\n",
                "                idmanage_df.loc[idmanage_df.blocked==False,'id_tuple']\r\n",
                "            )\r\n",
                "        ]\r\n",
                "        weather_df.reset_index(inplace=True,drop=True)\r\n",
                "\r\n",
                "        # change dtype of id_tuple col to tuple\r\n",
                "        weather_df.id_tuple = [\r\n",
                "            literal_eval(tup)\r\n",
                "            for tup in weather_df.id_tuple\r\n",
                "        ]\r\n",
                "\r\n",
                "        # get gemotry data\r\n",
                "        weather_df.loc[:,'sungeo'] = [\r\n",
                "            sun_geo(x[0],x[1],d)\r\n",
                "            for x,d\r\n",
                "            in zip(\r\n",
                "                weather_df.id_tuple,\r\n",
                "                weather_df.datetime\r\n",
                "            )]\r\n",
                "\r\n",
                "        # unpack the relevant data\r\n",
                "        weather_df.loc[:,'azimut'] = [x['azimut'] for x in weather_df['sungeo']]\r\n",
                "        weather_df.loc[:,'sunheight'] = [x['sunheight refracted'] for x in weather_df['sungeo']]\r\n",
                "        weather_df.loc[:,'albedo'] = [albedo(x[0],x[1]) for x in weather_df.id_tuple]\r\n",
                "        # drop the unpacked col\r\n",
                "        weather_df.drop('sungeo',inplace=True,axis=1)\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed1 = perf_counter() - starttime\r\n",
                "        # start new timer\r\n",
                "        starttime = perf_counter()\r\n",
                "\r\n",
                "\r\n",
                "        # get reference values from the settings and dcc.Store\r\n",
                "        # look up estimate of time since last rain in hours if there's no data available\r\n",
                "        rain_est = settings['losses']['rain_reference']\r\n",
                "        # look up earliest and latest available datetime\r\n",
                "        global dt_min, dt_max\r\n",
                "        dt_min, dt_max = pd.to_datetime(minmax_datetime['min']), pd.to_datetime(minmax_datetime['max'])\r\n",
                "\r\n",
                "        # the following code utilizes window functions to estimate the average amount of dust in the atmosphere\r\n",
                "        # we begin by looking up/estimating the time since the last and until the next rain event relative to a given datetime \r\n",
                "        # the last time tells us which values have to be included in the average\r\n",
                "        # the next time tells us at what point a new window has to be opened for the windows function to work properly\r\n",
                "\r\n",
                "        # create new col for weather_df to find the datetime of the next rain\r\n",
                "        # use the current time as next rain time if raining and then backfill, if there are still na values, use the latest possible datetime\r\n",
                "        weather_df.loc[:,'next_rain'] = pd.Series([\r\n",
                "            rain_prep(precip, datetime, dt_min)\r\n",
                "            for precip, datetime\r\n",
                "            in zip( weather_df.precip_1h,\r\n",
                "                    weather_df.datetime)\r\n",
                "            ])\r\n",
                "        weather_df.next_rain = weather_df.next_rain.fillna(method='bfill').fillna(dt_max)\r\n",
                "\r\n",
                "        # create new col for weather_df to find the datetime of the last rain\r\n",
                "        # use the current time as last rain time if raining, then forward fill\r\n",
                "        # if data is missing the last datetime will be imputed based on the rain estimate from the settings\r\n",
                "        weather_df.loc[:,'last_rain'] = pd.Series([\r\n",
                "            rain_prep(precip, datetime,dt_min,next=False)\r\n",
                "            for precip, datetime\r\n",
                "            in zip( weather_df.precip_1h,\r\n",
                "                    weather_df.datetime)\r\n",
                "            ])\r\n",
                "        weather_df.last_rain = weather_df.last_rain.fillna(method='ffill').fillna(dt_min-dt.timedelta(hours=rain_est))\r\n",
                "\r\n",
                "        # compute the time since the last rain in hours\r\n",
                "        # might be possible to simplify this, previous attempts took at least 10 times as long to complete\r\n",
                "        weather_df.loc[:,'hrs_since_rain'] = [\r\n",
                "            int((now-last).seconds/3600 + (now-last).days*24)\r\n",
                "            for now, last\r\n",
                "            in zip( weather_df.datetime,\r\n",
                "                    weather_df.last_rain)\r\n",
                "        ]\r\n",
                "\r\n",
                "        # finally compute the mean pollution (aod_mean) since the last rain event\r\n",
                "        weather_df.loc[:,'aod_mean'] = (\r\n",
                "            weather_df\r\n",
                "            # groupby location and next rain event to create correct windows\r\n",
                "            .groupby(['id_tuple','next_rain'])\r\n",
                "            # create expanding windows so only values before or at current datetime will be included in average\r\n",
                "            .expanding()\r\n",
                "            # get the mean values\r\n",
                "            .agg({'total_aod_550nm':'mean'})\r\n",
                "            # reset index\r\n",
                "            .reset_index()\r\n",
                "            # simplify column names\r\n",
                "            .drop('level_2',axis=1)\r\n",
                "            # keep only the mean values of the pollution to add to weather_df\r\n",
                "            .total_aod_550nm\r\n",
                "        )\r\n",
                "\r\n",
                "        # drop columns from weather_df which served their purpose\r\n",
                "        weather_df.drop(['total_aod_550nm','next_rain','last_rain','precip_1h'],axis=1,inplace=True)\r\n",
                "\r\n",
                "        \r\n",
                "        # change id_tuple col dtype back to string\r\n",
                "        weather_df.id_tuple = weather_df.id_tuple.astype(str)\r\n",
                "        # save the weather_df with the new data\r\n",
                "        weather_df.to_parquet('../data/02_intermediate/weather_pollution_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # stop the second timer\r\n",
                "        elapsed2 = perf_counter() - starttime\r\n",
                "\r\n",
                "        return (\r\n",
                "            ('59% done - Relative position of the sun calculated in '+f'{elapsed1:0.2f} seconds'),\r\n",
                "            ('63% done - Average pollution of the atmosphere calculated in '+f'{elapsed2:0.2f} seconds'),\r\n",
                "        )\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 162,
            "source": [
                "# after the geometric data and pollution averages are computed, estimate the energy need\r\n",
                "\r\n",
                "# imports\r\n",
                "from d02_intermediate.est_consumption import consumption_by_date, consumption_ampm\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('need_status','children'),\r\n",
                "    Input('sungeo_status','children'), # trigger when the sun geometry is computed\r\n",
                "    State('household_input','value'),\r\n",
                "    State('process_id_store','data'),\r\n",
                "    State('minmax_date_store','data')\r\n",
                ")\r\n",
                "def get_need(status,hh,process_id,minmax_dates):\r\n",
                "    if status == '':\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter()\r\n",
                "        # get the process id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "\r\n",
                "        # compute the energy needs per half day\r\n",
                "        # instantiate an empty list do collect dfs for each day\r\n",
                "        need_list = []\r\n",
                "        for day in (\r\n",
                "            pd.date_range(\r\n",
                "                minmax_dates['min'],\r\n",
                "                minmax_dates['max']\r\n",
                "            )\r\n",
                "        ):\r\n",
                "            # compute the daily total, use it to compute need for half days\r\n",
                "            a = consumption_by_date(\r\n",
                "                day,\r\n",
                "                settings['energy_need']['monthly_winter'],\r\n",
                "                settings['energy_need']['monthly_winter'],\r\n",
                "                hh\r\n",
                "            )\r\n",
                "            b = consumption_ampm(\r\n",
                "                a,\r\n",
                "                settings['energy_need']['night'] + settings['energy_need']['morning'],\r\n",
                "                settings['energy_need']['afternoon'] + settings['energy_need']['evening']\r\n",
                "            )\r\n",
                "            # save the half days in a df, collect the dfs\r\n",
                "            b = pd.DataFrame(b,columns=['need','pm'])\r\n",
                "            b['date']=day.date()\r\n",
                "            need_list.append(b)\r\n",
                "        # create a full df with all the need data\r\n",
                "        need_df = pd.concat(need_list,axis=0,ignore_index=True)\r\n",
                "        # set the index to make joining later on easier\r\n",
                "        need_df.set_index(['date','pm'],inplace=True)\r\n",
                "        # save the need_df\r\n",
                "        need_df.to_parquet('../data/02_intermediate/need_df_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed = perf_counter() -starttime\r\n",
                "        return ('66% done - Energy consumption estimated in '+f'{elapsed:0.2f} seconds')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 163,
            "source": [
                "# create suggestions for tilt and alignment values based on the need df\r\n",
                "# compute the angle of the sun beams to the panels with all suggested tilts & alignments\r\n",
                "\r\n",
                "# imports\r\n",
                "from d00_utils.tiltalign_adjust import tilt_options,align_options\r\n",
                "from math import atan, degrees\r\n",
                "from d01_data.get_weather_data import get_solar_irradiance\r\n",
                "from d02_intermediate.geometry3d import inc_angle_by_degrees as inc_angle\r\n",
                "from d02_intermediate.est_incidence_on_panel import radiation_incidence_on_panel as incidence_on_panel\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('angle_status','children'),\r\n",
                "    Input('need_status','children'), # trigger when XYZ\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def get_suggestions(status,process_id):\r\n",
                "    if status == '':\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter()\r\n",
                "        # get the process id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "        # look up power need settings\r\n",
                "        need_settings = settings['energy_need']\r\n",
                "        # generate suggestions for tilt and alignment values based on the need\r\n",
                "        # tilt suggestions are based on the ratio of need in the summer vs winter\r\n",
                "        tilt_suggs = tilt_options(\r\n",
                "            need_settings['monthly_summer'],\r\n",
                "            need_settings['monthly_winter']\r\n",
                "        )\r\n",
                "        # alignment suggestions are based on the consumption profile per day\r\n",
                "        align_suggs = align_options(\r\n",
                "            need_settings['night'],\r\n",
                "            need_settings['morning'],\r\n",
                "            need_settings['afternoon'],\r\n",
                "            need_settings['evening'],\r\n",
                "        )\r\n",
                "\r\n",
                "        # load weather_df\r\n",
                "        weather_df = pd.read_parquet('../data/02_intermediate/weather_pollution_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # add alignment and tilt options to each location and datetime\r\n",
                "\r\n",
                "        # add a tilt column which contains a string with the tilt options\r\n",
                "        weather_df.loc[:,'tilt'] = '-'.join([str(x) for x in tilt_suggs])\r\n",
                "        # split the string into a list\r\n",
                "        weather_df.tilt = weather_df.tilt.str.split('-')\r\n",
                "        # create a row for each item in the list\r\n",
                "        weather_df = weather_df.explode('tilt')\r\n",
                "        # repeat the process for the alignment suggestions\r\n",
                "        weather_df.loc[:,'alignment'] = '-'.join([str(x) for x in align_suggs])\r\n",
                "        weather_df.alignment = weather_df.alignment.str.split('-')\r\n",
                "        weather_df = weather_df.explode('alignment').reset_index(drop=True)\r\n",
                "        # change the dtype of the tilts and alignments to integer\r\n",
                "        # this will lose a bit of details but it's fine\r\n",
                "        weather_df.tilt = weather_df.tilt.astype('int')\r\n",
                "        weather_df.alignment = weather_df.alignment.astype('int')\r\n",
                "\r\n",
                "        # compute the angle of incidence for each location and tilt alignment combination\r\n",
                "        # compute angle of incidence\r\n",
                "        weather_df.loc[:,'angle'] = [inc_angle(\r\n",
                "                a,\r\n",
                "                s,\r\n",
                "                tilt,\r\n",
                "                alignment\r\n",
                "            ) \r\n",
                "            for a,s,tilt,alignment\r\n",
                "            in zip(\r\n",
                "                weather_df.azimut,\r\n",
                "                weather_df.sunheight,\r\n",
                "                weather_df.tilt,\r\n",
                "                weather_df.alignment)]\r\n",
                "        # compute the solar irradiance for each date\r\n",
                "        weather_df.loc[:,'solar_irradiance'] = [get_solar_irradiance(x) for x in weather_df.date]\r\n",
                "        # compute the incidence on the panel\r\n",
                "        weather_df.loc[:,'incidence'] = [\r\n",
                "            incidence_on_panel(glo,dir,dif,irr,sh,tilt,albedo,angle)\r\n",
                "            for glo,dir,dif,irr,sh,tilt,albedo,angle\r\n",
                "            in zip(\r\n",
                "                weather_df.global_rad,\r\n",
                "                weather_df.direct_rad,\r\n",
                "                weather_df.diffuse_rad,\r\n",
                "                weather_df.solar_irradiance,\r\n",
                "                weather_df.sunheight,\r\n",
                "                weather_df.tilt,\r\n",
                "                weather_df.albedo,\r\n",
                "                weather_df.angle\r\n",
                "            )]\r\n",
                "\r\n",
                "        # drop used data\r\n",
                "        weather_df.drop(['direct_rad','diffuse_rad','solar_irradiance','sunheight','albedo','angle','azimut'],axis=1,inplace=True)\r\n",
                "\r\n",
                "        # save the weather_df\r\n",
                "        weather_df.to_parquet('../data/02_intermediate/weather_df_angles_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed = perf_counter() -starttime\r\n",
                "        return ('81% done - Angle of sun beams on solar panel calculated in '+f'{elapsed:0.2f} seconds')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 164,
            "source": [
                "# compute the losses and output per m² panel area for each location, tilt and alignment\r\n",
                "\r\n",
                "# imports\r\n",
                "from d02_intermediate.weather_int import est_snowdepth_panel, est_paneltemp, loss_by_snow, loss_by_temp, est_soiling_loss_data\r\n",
                "\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('output_status','children'),\r\n",
                "    Input('angle_status','children'), # trigger when XYZ\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def get_need(status,process_id):\r\n",
                "    if status == '':\r\n",
                "        return ''\r\n",
                "    else:\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter()\r\n",
                "        # get the process id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "        # load weather_df\r\n",
                "        weather_df = pd.read_parquet('../data/02_intermediate/weather_df_angles_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # compute loss by snow on panel\r\n",
                "        weather_df.loc[:,'loss_snow'] = \\\r\n",
                "            [loss_by_snow(\\\r\n",
                "                est_snowdepth_panel(x)) \r\n",
                "            for x\r\n",
                "            in weather_df.snow_depth]\r\n",
                "\r\n",
                "        # look up technical losses and panel_efficiency\r\n",
                "        panel_info = settings['panel_info']\r\n",
                "\r\n",
                "        # compute loss by temperature of the panel\r\n",
                "        weather_df.loc[:,'loss_temp'] = \\\r\n",
                "            [loss_by_temp(\\\r\n",
                "                est_paneltemp(temp, wind),panel_info['temperature_coefficient']) \r\n",
                "            for temp, wind\r\n",
                "            in zip(\r\n",
                "                weather_df.t_2m,\r\n",
                "                weather_df.wind_speed_10m) ]\r\n",
                "\r\n",
                "        # compute loss by soiling on panel\r\n",
                "        from d02_intermediate.weather_int import est_soil_loss_value as slvalue\r\n",
                "        weather_df.loc[:,'loss_soiling'] = \\\r\n",
                "            [ slvalue(tilt,hrs_since,aod_mean)\r\n",
                "            for tilt, hrs_since, aod_mean\r\n",
                "            in zip(weather_df.tilt,weather_df.hrs_since_rain,weather_df.aod_mean)]\r\n",
                "\r\n",
                "        # drop used data \r\n",
                "        weather_df.drop(['snow_depth', 'wind_speed_10m','t_2m','aod_mean','hrs_since_rain'],axis=1,inplace=True)\r\n",
                "\r\n",
                "        # compute a loss column with the total losses by all high variance factors combined\r\n",
                "        weather_df.loc[:,'loss_highvar'] = [\r\n",
                "            1-(1-l1)*(1-l2)*(1-l3)\r\n",
                "            for l1,l2,l3 \r\n",
                "            in zip(\r\n",
                "                weather_df.loss_snow,\r\n",
                "                weather_df.loss_soiling,\r\n",
                "                weather_df.loss_temp\r\n",
                "            )\r\n",
                "        ]\r\n",
                "\r\n",
                "\r\n",
                "        # look up technical loss\r\n",
                "        technical_loss = settings['losses']['loss_technical']\r\n",
                "        # compute an output col with output after highvar losses in w/m²\r\n",
                "        weather_df.loc[:,'output'] = [\r\n",
                "            incidence*(1-losses)* panel_info['efficiency'] * (1-technical_loss)\r\n",
                "            for incidence, losses\r\n",
                "            in zip(\r\n",
                "                weather_df.incidence,\r\n",
                "                weather_df.loss_highvar\r\n",
                "            )\r\n",
                "        ]\r\n",
                "\r\n",
                "        # save the weather_df\r\n",
                "        weather_df.to_parquet('../data/03_processed/weather_df_output_{}'.format(process_id))\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed = perf_counter() -starttime\r\n",
                "        return ('90% done - Possible outputs calculated in '+f'{elapsed:0.2f} seconds')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 165,
            "source": [
                "# create custom aggregation function\r\n",
                "# will be used to find the smallest total panel area that still leads to the requested ratio of self sufficient days\r\n",
                "def agg_percentile_finder(series):\r\n",
                "    '''\r\n",
                "    returns the percentile, only works if the series is sorted (ascending)'''\r\n",
                "    # find the position of the value corresponding to the percentile\r\n",
                "    pos = ceil(len(series)*settings['project_info']['sufficiency_ratio'])-1\r\n",
                "    return series.iloc[pos]\r\n",
                "\r\n",
                "# func to compute kwp from total panel size\r\n",
                "def area2kwp(area):\r\n",
                "    '''returns kwp provided by panel size'''\r\n",
                "    return (\r\n",
                "        area\r\n",
                "        *settings['panel_info']['rated_capacity_kw']\r\n",
                "        /settings['panel_info']['panel_area_sqm']\r\n",
                "    )\r\n",
                "\r\n",
                "# func to compute total panel area\r\n",
                "def total_area(need,output):\r\n",
                "    if output == 0:\r\n",
                "        return 1000_000\r\n",
                "    else:\r\n",
                "        return need/output*1000"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 166,
            "source": [
                "# find the optimal tilts, alignments and locations for the highest possible output or high roi\r\n",
                "\r\n",
                "# imports\r\n",
                "from d03_processing.cost_and_earnings import compute_finances, optimal_combination, setup_cost\r\n",
                "from math import ceil\r\n",
                "from d07_visualisation.create_geometries import create_polygons\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('maxout_status','children'),\r\n",
                "    Output('maxroi_status','children'),\r\n",
                "    Input('output_status','children'), # trigger when XYZ\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def get_need(status,process_id):\r\n",
                "    if status == '':\r\n",
                "        return '',''\r\n",
                "    else:\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter()\r\n",
                "        # get the process id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "        # load weather_df \r\n",
                "        weather_df = pd.read_parquet('../data/03_processed/weather_df_output_{}'.format(process_id))\r\n",
                "\r\n",
                "\r\n",
                "        # aggregation dict, will be used to find the optimal tilts, alignments and locations\r\n",
                "        # choose aggregation method for relevant columns\r\n",
                "        sums = ['output']\r\n",
                "        means = ['loss_snow','loss_temp','loss_soiling','loss_highvar']\r\n",
                "        # construct a dict from the list\r\n",
                "        agg_dict = {\r\n",
                "            **dict.fromkeys(sums,'sum'),\r\n",
                "            **dict.fromkeys(means,'mean')\r\n",
                "        }\r\n",
                "        # aggregate weather df with the aggregation dict, save with new name to keep the former weather_df as it is\r\n",
                "        weather_df_max =(\r\n",
                "            weather_df.copy()\r\n",
                "            # groupby id_tuple tilt and alignment to compare between those\r\n",
                "            .groupby(['id_tuple','tilt','alignment'])\r\n",
                "            .agg(agg_dict)\r\n",
                "        )\r\n",
                "        # keep separate loss_df, will be added back to the df later on \r\n",
                "        loss_df = (\r\n",
                "            weather_df_max.copy()\r\n",
                "            # keep only the relevant columns\r\n",
                "            .iloc[:,0:8]\r\n",
                "            .drop('output',axis=1)\r\n",
                "        )\r\n",
                "\r\n",
                "        # save the tilt-alignment combination corresponding to the highest total output for each location: best\r\n",
                "        max_output_index = list(\r\n",
                "            weather_df_max.copy()\r\n",
                "            .groupby(['id_tuple'])\r\n",
                "            .agg({'output':'idxmax'})\r\n",
                "            .output\r\n",
                "        )\r\n",
                "\r\n",
                "        # load the need_df\r\n",
                "        need_df = pd.read_parquet('../data/02_intermediate/need_df_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # create a dataframe with the highest possible output for each location\r\n",
                "        max_out_df=(\r\n",
                "            optimal_combination(\r\n",
                "                weather_df,\r\n",
                "                max_output_index,\r\n",
                "                'output',\r\n",
                "                need_df,loss_df\r\n",
                "            )\r\n",
                "        )\r\n",
                "\r\n",
                "        # look up financial settings and the scale\r\n",
                "        fin_settings = settings['financial']\r\n",
                "        project_info = settings['project_info']\r\n",
                "        \r\n",
                "        # add col with the total area\r\n",
                "        max_out_df.loc[:,'panel_area'] = project_info['total_panel_size'][scale]\r\n",
                "        # compute financial info for the locations and tilt and alignment combinations\r\n",
                "        max_out_df = compute_finances(\r\n",
                "            max_out_df,\r\n",
                "            settings['project_info']['max_kwp'][scale],\r\n",
                "            fin_settings['kwh_cost'],\r\n",
                "            compensation,\r\n",
                "            output_col='output'\r\n",
                "        )\r\n",
                "        # round the output col\r\n",
                "        max_out_df.output_total_daily= [\r\n",
                "            round(output,2) for output in max_out_df.output_total_daily\r\n",
                "        ]\r\n",
                "        # add a col for the setup cost\r\n",
                "        max_out_df.loc[:,'installation_cost'] = setup_cost(29.5)\r\n",
                "\r\n",
                "        # add ratio between savings and installation cost\r\n",
                "        max_out_df.loc[:,'savings_cost_ratio'] = [\r\n",
                "            savings/cost\r\n",
                "            for savings, cost\r\n",
                "            in zip(\r\n",
                "                max_out_df.savings_daily,\r\n",
                "                max_out_df.installation_cost) \r\n",
                "        ]\r\n",
                "\r\n",
                "        # calculate the installment payment if the power system would be partially (80%) financed by taking out a loan\r\n",
                "        # loan is paid back over 20 years with 4.5% interest\r\n",
                "        max_out_df.loc[:,'installment_payment_monthly'] = round(max_out_df.installation_cost*1000/197.6,2)\r\n",
                "        # compute the real savings after the monthly installment\r\n",
                "        max_out_df.loc[:,'savings_after_installment_monthly'] = [\r\n",
                "            daily*30 - installment\r\n",
                "            for daily, installment \r\n",
                "            in zip(\r\n",
                "                max_out_df.savings_daily,\r\n",
                "                max_out_df.installment_payment_monthly\r\n",
                "            )\r\n",
                "        ]\r\n",
                "        # compute the yearly return on investment in comparison to the investment of 20% of the whole setup cost\r\n",
                "        max_out_df.loc[:,'return_yearly'] = [\r\n",
                "            round(12*savings/(investment*10/5),1)\r\n",
                "            for savings, investment\r\n",
                "            in zip(\r\n",
                "                max_out_df.savings_after_installment_monthly,\r\n",
                "                max_out_df.installation_cost\r\n",
                "            )\r\n",
                "        ]\r\n",
                "\r\n",
                "        # use max out to create a geojson file with polygons to plot\r\n",
                "        # copy the first col of max_out_df, content doesn't matter\r\n",
                "        to_geojson = max_out_df[[max_out_df.columns[0]]].copy()\r\n",
                "        # make sure the index consists of tuples\r\n",
                "        to_geojson.index = [\r\n",
                "            literal_eval(tup) \r\n",
                "            for tup in to_geojson.index\r\n",
                "        ]\r\n",
                "        to_geojson.index.name = 'id_tuple'\r\n",
                "        # add a geometry col which contains polygons, these will be exported\r\n",
                "        to_geojson = create_polygons(to_geojson).reset_index()\r\n",
                "        print(to_geojson.columns)\r\n",
                "        # set coordinate reference system\r\n",
                "        crs = 'epsg:4326'\r\n",
                "        # turn the dataframe into a geodataframe, this makes it possible to create a geojson file\r\n",
                "        to_geojson = gpd.GeoDataFrame(to_geojson, crs=crs, geometry=to_geojson.geometry)\r\n",
                "        # create new id_col for easier plotting\r\n",
                "        to_geojson.loc[:,'id'] = to_geojson.id_tuple.astype(str)\r\n",
                "        # create a geojson file\r\n",
                "        polygon_json = gpd.GeoSeries(to_geojson.geometry).__geo_interface__\r\n",
                "        # save the geojson\r\n",
                "        path = '../references/polygons_'+process_id+'.json'\r\n",
                "        with open(path,'w') as f:\r\n",
                "            json.dump(polygon_json,f)\r\n",
                "\r\n",
                "\r\n",
                "        # save max_out_df\r\n",
                "        max_out_df.to_parquet('../data/03_processed/max_out_{}.pqt'.format(process_id))\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed1 = perf_counter() -starttime\r\n",
                "        # start a new timer\r\n",
                "        starttime = perf_counter()\r\n",
                "\r\n",
                "        # compute the optimal combination of tilt and alignment to reach the requested ratio of \r\n",
                "        # self sufficient half-days with the minimal total panel area\r\n",
                "        # aggregrate based on the agg dict, join energy need data to weather_df\r\n",
                "        weather_df = (\r\n",
                "            weather_df\r\n",
                "            # groupby by tilt and alignment to find the best combination, groupby date and pm to compare half days\r\n",
                "            .groupby(['id_tuple','date','pm','alignment','tilt'])\r\n",
                "            # same aggregation as for the max out df\r\n",
                "            .agg(agg_dict)\r\n",
                "            .reset_index()\r\n",
                "            .join(need_df,on=['date','pm'])\r\n",
                "        )\r\n",
                "        # compute the needed total area which would lead to the requested ratio of self sufficient half-days\r\n",
                "        weather_df.loc[:,'area_indicator'] = [total_area(need,output) for need,output in zip(weather_df.need,weather_df.output)]\r\n",
                "\r\n",
                "        # create an indicator_df which stores what area would be required to reach the requested ratio of self sufficient half-days\r\n",
                "        indicator_df = (\r\n",
                "            weather_df.copy()\r\n",
                "            .set_index('id_tuple')\r\n",
                "            # sort so the custom aggregator works fine\r\n",
                "            .sort_values(['tilt','alignment','area_indicator'])\r\n",
                "            .groupby(['id_tuple','tilt','alignment'])\r\n",
                "            # find the requested percentile for each tilt-alignment combination\r\n",
                "            .agg({'area_indicator':agg_percentile_finder})\r\n",
                "        )\r\n",
                "\r\n",
                "        # save the index of the rows with the smallest area for each location\r\n",
                "        # this index includes the tilt and alignment values\r\n",
                "        smallest_size_idx = list(\r\n",
                "            indicator_df\r\n",
                "            # find the index with the lowest area indicator per location\r\n",
                "            # index is made up of tilt and alignment\r\n",
                "            .groupby(['id_tuple'])\r\n",
                "            .agg('idxmin')\r\n",
                "            .area_indicator\r\n",
                "        )\r\n",
                "\r\n",
                "        # get the df optimized for self sufficiency \r\n",
                "        smallest_size_df=(\r\n",
                "            optimal_combination(\r\n",
                "                weather_df,\r\n",
                "                smallest_size_idx,\r\n",
                "                'output',\r\n",
                "                need_df,loss_df,indicator_df\r\n",
                "            )\r\n",
                "        )\r\n",
                "\r\n",
                "        # compute financial data for the smallest panel installations\r\n",
                "        smallest_size_df = compute_finances(\r\n",
                "            smallest_size_df,\r\n",
                "            settings['project_info']['max_kwp'][scale],\r\n",
                "            fin_settings['kwh_cost'],\r\n",
                "            compensation,\r\n",
                "            output_col='output',\r\n",
                "            panel_size_col='area_indicator'\r\n",
                "        )\r\n",
                "\r\n",
                "        # join indicator df to smallest_size_df to get the info about optimal tilt and alignment combinations\r\n",
                "        smallest_size_df=smallest_size_df.join(indicator_df.loc[smallest_size_idx])\r\n",
                "        # add installation cost\r\n",
                "        smallest_size_df.loc[:,'installation_cost'] = [\r\n",
                "            setup_cost(area2kwp(size))\r\n",
                "            for size\r\n",
                "            in smallest_size_df.area_indicator\r\n",
                "        ]\r\n",
                "\r\n",
                "        # add ratio between savings and installation cost\r\n",
                "        smallest_size_df.loc[:,'savings_cost_ratio'] = [\r\n",
                "            savings/cost\r\n",
                "            for savings, cost\r\n",
                "            in zip(\r\n",
                "                smallest_size_df.savings_daily,\r\n",
                "                smallest_size_df.installation_cost) \r\n",
                "        ]\r\n",
                "\r\n",
                "        # calculate the installment payment if the power system would be partially (80%) financed by taking out a loan\r\n",
                "        # loan is paid back over 20 years with 4.5% interest\r\n",
                "        smallest_size_df.loc[:,'installment_payment_monthly'] = round(smallest_size_df.installation_cost*1000/197.6,2)\r\n",
                "        # compute the real savings after the monthly installment\r\n",
                "        smallest_size_df.loc[:,'savings_after_installment_monthly'] = [\r\n",
                "            daily*30 - installment\r\n",
                "            for daily, installment \r\n",
                "            in zip(\r\n",
                "                smallest_size_df.savings_daily,\r\n",
                "                smallest_size_df.installment_payment_monthly\r\n",
                "            )\r\n",
                "        ]\r\n",
                "        # compute the yearly return on investment in comparison to the investment of 20% of the whole setup cost\r\n",
                "        smallest_size_df.loc[:,'return_yearly'] = [\r\n",
                "            round(12*savings/(investment*10/5),1)\r\n",
                "            for savings, investment\r\n",
                "            in zip(\r\n",
                "                smallest_size_df.savings_after_installment_monthly,\r\n",
                "                smallest_size_df.installation_cost\r\n",
                "            )\r\n",
                "        ]\r\n",
                "\r\n",
                "        # reset and re-set the index so that only id_tuple is in the index\r\n",
                "        smallest_size_df = smallest_size_df.reset_index().set_index('id_tuple')\r\n",
                "        # save smallest_size_df \r\n",
                "        smallest_size_df.to_parquet('../data/03_processed/max_roi_{}.pqt'.format(process_id))\r\n",
                "        elapsed2 = perf_counter()-starttime\r\n",
                "\r\n",
                "        return (\r\n",
                "            ('94% done - Highest possible output for each location analysed in '+f'{elapsed1:0.2f} seconds'),\r\n",
                "            ('95% done - Highest possible ROI for each location analysed in '+f'{elapsed1:0.2f} seconds'),\r\n",
                "        )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 167,
            "source": [
                "# show the plots\r\n",
                "\r\n",
                "# imports\r\n",
                "import plotly.express as px\r\n",
                "\r\n",
                "@app.callback(\r\n",
                "    Output('plotted_status','children'), # inform the user when the plots are available\r\n",
                "    # output the plots\r\n",
                "    Output('map_graph','figure'),\r\n",
                "   \r\n",
                "    Input('maxout_status','children'), # trigger when the data is analyzed\r\n",
                "    Input('to_plot_radio','value'), # also trigger when a new output is chosen\r\n",
                "    State('process_id_store','data')\r\n",
                ")\r\n",
                "def get_need(status,to_plot,process_id):\r\n",
                "    if status == '':\r\n",
                "        return '', no_fig\r\n",
                "    else:\r\n",
                "        # start the timer\r\n",
                "        starttime = perf_counter()\r\n",
                "        # get the process id\r\n",
                "        process_id = process_id[0]\r\n",
                "\r\n",
                "        # load the dfs and the geodata\r\n",
                "        maxout_df = pd.read_parquet('../data/03_processed/max_out_{}.pqt'.format(process_id)).reset_index().reset_index()\r\n",
                "        maxroi_df = pd.read_parquet('../data/03_processed/max_roi_{}.pqt'.format(process_id)).reset_index().reset_index()\r\n",
                "        with open('../references/polygons_{}.json'.format(process_id), 'r') as f:\r\n",
                "            geojson = json.load(f)\r\n",
                "        # create dict to choose correct df to plot\r\n",
                "        # dict includes a list for each to_plot value, first element is the df, second is the name of the col which should be plotted\r\n",
                "        to_plot_dict = {\r\n",
                "            'maxout':[maxout_df,'output_total_daily'],\r\n",
                "            'maxroi':[maxroi_df,'return_yearly'],\r\n",
                "        }\r\n",
                "\r\n",
                "        # begin by creating the map\r\n",
                "        map_fig = px.choropleth_mapbox(to_plot_dict[to_plot][0], geojson=geojson,\r\n",
                "                                locations='index', color=to_plot_dict[to_plot][1],\r\n",
                "                                color_continuous_scale=\"bluered\",\r\n",
                "                                #    range_color=(0.1, .11),\r\n",
                "                                \r\n",
                "                                mapbox_style=\"open-street-map\",\r\n",
                "                                zoom=9, center = {\r\n",
                "                                    \"lat\": (\r\n",
                "                                        # get the middle latitude\r\n",
                "                                        geojson['features'][int(len(geojson['features'])/2)]['bbox'][1]\r\n",
                "                                    ),\r\n",
                "                                    \"lon\":(\r\n",
                "                                        geojson['features'][int(len(geojson['features'])/2)]['bbox'][0]\r\n",
                "                                    )\r\n",
                "                                },\r\n",
                "                                labels={\r\n",
                "                                    'output_total_daily':'Maximum electricity<br>output per day<br>in kWh',\r\n",
                "                                    'return_yearly':'Yearly return<br>on investment<br>in percent'\r\n",
                "                                },\r\n",
                "                                opacity=0.65,\r\n",
                "                                # hover_data={\r\n",
                "                                #     'output_total_daily':'Maximum electricity output per day',\r\n",
                "                                #     'return_yearly':'Yearly return on investment'\r\n",
                "                                # },\r\n",
                "                                height=1080, width=1920\r\n",
                "                                )\r\n",
                "        map_fig.update_layout(\r\n",
                "            margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\r\n",
                "        )\r\n",
                "\r\n",
                "        hist_fig = px.scatter(\r\n",
                "            to_plot_dict[to_plot][0],\r\n",
                "            x='loss_temp',\r\n",
                "            y='loss_soiling',\r\n",
                "            color='loss_highvar',\r\n",
                "            color_continuous_scale=\"bluered\",\r\n",
                "            height = 600\r\n",
                "        )\r\n",
                "\r\n",
                "        # stop the timer\r\n",
                "        elapsed = perf_counter() -starttime\r\n",
                "        return ('100% done - Plots generated in '+f'{elapsed:0.2f} seconds'), map_fig"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 168,
            "source": [
                "# run the server\r\n",
                "# server can also be accessed via browser (http://127.0.0.1:8050/)\r\n",
                "app.run_server(mode='jupyterlab',debug=True,port=8050)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\Techie\\anaconda3\\envs\\mapenv\\lib\\site-packages\\jupyter_dash\\jupyter_app.py:139: UserWarning:\n",
                        "\n",
                        "The 'environ['werkzeug.server.shutdown']' function is deprecated and will be removed in Werkzeug 2.1.\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "clicks reset\n",
                        "locref triggered but no update\n",
                        "get weather triggered, return empty string\n",
                        "loc ref block triggered, nothing returned\n",
                        "no weather data, waiting\n",
                        "locref triggered and work done\n",
                        "available weather data will be loaded. change use_available or process_id to request new data\n",
                        "weather checked, id: ['duesseldorf_200']\n",
                        "ids df is a <class 'pandas.core.frame.DataFrame'>\n",
                        "pre point\n",
                        "tuple is in str\n",
                        "now its a tuple\n",
                        "geo created\n",
                        "geoblock done\n",
                        "blocks updated\n",
                        "blocks by locref done\n",
                        "weather pivoted\n",
                        "timer started for requests\n",
                        "topo available\n",
                        "topo block started\n",
                        "Index(['id_tuple', 'tilt', 'alignment', 'date', 'pm', 'output', 'need',\n",
                        "       'loss_snow', 'loss_temp', 'loss_soiling', 'loss_highvar', 'panel_area'],\n",
                        "      dtype='object')\n",
                        "Index(['id_tuple', 'loss_snow', 'geometry'], dtype='object')\n",
                        "Index(['id_tuple', 'tilt', 'alignment', 'date', 'pm', 'output', 'need',\n",
                        "       'loss_snow', 'loss_temp', 'loss_soiling', 'loss_highvar',\n",
                        "       'area_indicator'],\n",
                        "      dtype='object')\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('mapenv': conda)"
        },
        "interpreter": {
            "hash": "c7493435b741875e47357b1d4e2959b9ee7bbd8a014d8aa9e12beae2b4f843e2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}